{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flash Attention in Torch, Numba and Cuda\n",
    "\n",
    "\n",
    "We implement in three different ways the Flash Attention forward algorithm from the [Flash Attention 2 paper](https://arxiv.org/pdf/2307.08691). Namely using:\n",
    "\n",
    "1. Torch\n",
    "2. Numba\n",
    "3. Cuda\n",
    "\n",
    "- We build the kernel for `d=128` and design it so that it computes the full attention with each block running one iteration of the flash attention outer loop.\n",
    "\n",
    "- We then do some basic performance analysis, showing that on small matrices it performs like torch sdpa and disuss register spiling profiling.\n",
    "\n",
    "- Finally we run our custom kernel with [Cuda-python](https://developer.nvidia.com/cuda-python) and [Thunder](https://lightning.ai/docs/thunder/latest/).\n",
    "\n",
    "\n",
    "\n",
    "![./flash_attention_fwd.png](./flash_attention_fwd.png)\n",
    "\n",
    "\n",
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max absolute difference:\n",
      "O:  tensor(1.1921e-07, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import numba\n",
    "from numba.cuda import as_cuda_array as ca\n",
    "from pathlib import Path\n",
    "import math\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "from utils import load_cuda, get_sig, print_cuda_info\n",
    "\n",
    "TEST_DIMS = [\n",
    "    (32, 32),\n",
    "    (128, 64),\n",
    "    (512, 512),\n",
    "    (1024, 512),\n",
    "]\n",
    "\n",
    "def get_loaded_cuda_module(fname, verbose=False):\n",
    "    cuda_src_path = f\"./{fname}.cu\"\n",
    "    torch_src_path = f\"./torch_extension_template.cu\"\n",
    "    cuda_src = Path(cuda_src_path).read_text()\n",
    "    cuda_src += Path(torch_src_path).read_text()\n",
    "    cuda_src = cuda_src.replace(\"your_function_name\", fname)\n",
    "    cpp_src = get_sig(fname, cuda_src)\n",
    "    return load_cuda(cuda_src, cpp_src, [fname], verbose=verbose)\n",
    "\n",
    "\n",
    "def check_close(O, O_expected, L=None, L_expected=None, atol=5*1e-5):\n",
    "    O_diff = (O - O_expected).abs().max()\n",
    "    print(\"Max absolute difference:\")\n",
    "    if atol:\n",
    "        assert O_diff < atol, f\"O diff too large: {O_diff} > {atol=}\"\n",
    "    print(\"O: \", O_diff)\n",
    "    if L is not None:\n",
    "        L_diff = (L.squeeze() - L_expected).abs().max()\n",
    "        if atol:\n",
    "            assert L_diff < atol, f\"L diff too large: {L_diff} > {atol=}\"\n",
    "        print(\"L: \", L_diff)\n",
    "\n",
    "# Test tensors\n",
    "def get_test_tensors(N_inp, N_out, d):\n",
    "    Q = torch.randn(N_out, d).contiguous().to(\"cuda\")\n",
    "    K = torch.randn(N_inp, d).contiguous().to(\"cuda\")\n",
    "    V = torch.randn(N_inp, d).contiguous().to(\"cuda\")\n",
    "    scaling = 1.0 / math.sqrt(d)\n",
    "\n",
    "    # Get expected O\n",
    "    O_expected = torch.nn.functional.scaled_dot_product_attention(Q, K, V)\n",
    "    S = (Q @ K.T) * scaling  # shape: (N_out, N_inp)\n",
    "    L_expected = torch.logsumexp(S, dim=-1)\n",
    "    return Q, K, V, scaling, O_expected, L_expected\n",
    "\n",
    "# Check test tensors\n",
    "N_inp = 512\n",
    "N_out = 512\n",
    "d = 128\n",
    "\n",
    "Q, K, V, scaling, O_expected, L_expected = get_test_tensors(N_inp, N_out, d)\n",
    "check_close(O=torch.softmax(Q @ K.T * scaling, dim=-1) @ V, O_expected=O_expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flash_attention_torch(Q, K, V, O, L, N_inp, N_out, d) -> None:\n",
    "    B_c = 16\n",
    "    B_r = 16\n",
    "    T_c = (N_inp + B_c - 1) // B_c\n",
    "    T_r = (N_out + B_r - 1) // B_r\n",
    "\n",
    "    scaling = 1 / math.sqrt(d)\n",
    "\n",
    "    # Q and O L split into T_r; K, V in T_c blocks\n",
    "    for i in range(T_r):\n",
    "        Q_i = Q[i * B_r : (i + 1) * B_r]\n",
    "        O_i = torch.zeros(B_r, d)\n",
    "        L_i = torch.zeros(B_r, 1)\n",
    "        m_i = torch.full((B_r, 1), -math.inf)\n",
    "        last_m_i = m_i\n",
    "        for j in range(T_c):\n",
    "            K_j = K[j * B_c : (j + 1) * B_c]\n",
    "            V_j = V[j * B_c : (j + 1) * B_c]\n",
    "            S_i = scaling * (Q_i @ K_j.T)\n",
    "            m_i = torch.maximum(m_i, S_i.max(dim=-1, keepdim=True).values)\n",
    "            P_i = torch.exp(S_i - m_i)\n",
    "            L_i = torch.exp(last_m_i - m_i) * L_i + P_i.sum(dim=-1, keepdim=True)\n",
    "            O_i = torch.exp(last_m_i - m_i) * O_i + P_i @ V_j\n",
    "            last_m_i = m_i\n",
    "        O_i = (1.0 / L_i) * O_i\n",
    "        L_i = m_i + torch.log(L_i)\n",
    "        O[i * B_r : (i + 1) * B_r] = O_i\n",
    "        L[i * B_r : (i + 1) * B_r] = L_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max absolute difference:\n",
      "O:  tensor(3.2037e-07, device='cuda:0')\n",
      "L:  tensor(9.5367e-07, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "O_torch_loop = torch.zeros(N_out, d)\n",
    "L_torch_loop = torch.zeros(N_out, 1)\n",
    "\n",
    "flash_attention_torch(Q.to(\"cpu\"), K.to(\"cpu\"), V.to(\"cpu\"), O_torch_loop, L_torch_loop, N_inp, N_out, d)\n",
    "\n",
    "check_close(\n",
    "    O_torch_loop.to(\"cuda\"), \n",
    "    O_expected,\n",
    "    L_torch_loop.to(\"cuda\"),\n",
    "    L_expected\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numba\n",
    "\n",
    "Tiling strategy: threads cooperate loading shared arrays in each `i, j` loop and write their own entries of `l_i, m_i, O_i`\n",
    "\n",
    "### All arrays in shared memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.cuda.jit\n",
    "def flash_attention_numba_all_smem(Q, K, V, scaling: numba.float32, L, O, N_out, N_inp):\n",
    "    B_c = 16\n",
    "    B_r = 16\n",
    "    T_c = (N_inp + B_c - 1) // B_c\n",
    "    T_r = (N_out + B_r - 1) // B_r\n",
    "    inp_dtype = K.dtype\n",
    "    tid_x = numba.cuda.threadIdx.x\n",
    "    tid_y = numba.cuda.threadIdx.y\n",
    "\n",
    "    Q_i = numba.cuda.shared.array((B_r, d), inp_dtype)\n",
    "    K_j = numba.cuda.shared.array((B_c, d), inp_dtype)\n",
    "    V_j = numba.cuda.shared.array((B_c, d), inp_dtype)\n",
    "    S = numba.cuda.shared.array((B_r, B_c), inp_dtype)\n",
    "    # These can be in registers but wont fit too large\n",
    "    l_i = numba.cuda.shared.array((B_r,), inp_dtype)\n",
    "    m_i = numba.cuda.shared.array((B_r,), inp_dtype)\n",
    "    O_i = numba.cuda.shared.array((B_r, d), inp_dtype)\n",
    "\n",
    "                 \n",
    "    for i in range(T_r):\n",
    "        for ii in range(tid_y, B_r, numba.cuda.blockDim.y):\n",
    "            for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                Q_i[ii, dd] = Q[ii + i * B_r, dd]\n",
    "                O_i[ii, dd] = 0\n",
    "            l_i[ii] = 0\n",
    "            m_i[ii] = -math.inf\n",
    "        numba.cuda.syncthreads()\n",
    "\n",
    "        for j in range(T_c):\n",
    "            for jj in range(tid_y, B_c, numba.cuda.blockDim.y):\n",
    "                for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                    K_j[jj, dd] = K[jj + j * B_c, dd]\n",
    "                    V_j[jj, dd] = V[jj + j * B_c, dd]\n",
    "\n",
    "            # S[ii][jj] = scaling * (Q_i @ K_j.T)\n",
    "            numba.cuda.syncthreads()\n",
    "            for ii in range(tid_x, B_r, numba.cuda.blockDim.x):\n",
    "                for jj in range(tid_y, B_c, numba.cuda.blockDim.y):\n",
    "                    S_ij = 0\n",
    "                    for dd in range(d):\n",
    "                        S_ij += Q_i[ii, dd] * K_j[jj, dd]\n",
    "                    S_ij = scaling * S_ij\n",
    "                    S[ii][jj] = S_ij\n",
    "\n",
    "            numba.cuda.syncthreads()\n",
    "            for ii in range(tid_y, B_r, numba.cuda.blockDim.y):\n",
    "                m = m_i[ii]\n",
    "                last_m = m\n",
    "                for jj in range(B_c):\n",
    "                    m = max(m, S[ii][jj])\n",
    "                m_i[ii] = m\n",
    "                l = math.exp(last_m - m) * l_i[ii]\n",
    "\n",
    "                for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                    O_i[ii, dd] *= math.exp(last_m - m)\n",
    "                for jj in range(B_c):\n",
    "                    P_ij = math.exp(S[ii][jj] - m)\n",
    "                    l += P_ij\n",
    "                    for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                        O_i[ii, dd] += P_ij * V_j[jj, dd]\n",
    "                l_i[ii] = l\n",
    "                \n",
    "        for ii in range(tid_y, B_r, numba.cuda.blockDim.y):  \n",
    "            for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                O[ii + i * B_r, dd] = O_i[ii, dd] / l_i[ii]\n",
    "            L[ii + i * B_r] = m_i[ii] + math.log(l_i[ii])\n",
    "        \n",
    "        numba.cuda.syncthreads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max absolute difference:\n",
      "O:  tensor(7.1526e-07, device='cuda:0')\n",
      "L:  tensor(4.7684e-07, device='cuda:0')\n",
      "Max absolute difference:\n",
      "O:  tensor(3.5763e-07, device='cuda:0')\n",
      "L:  tensor(9.5367e-07, device='cuda:0')\n",
      "Max absolute difference:\n",
      "O:  tensor(4.1723e-07, device='cuda:0')\n",
      "L:  tensor(9.5367e-07, device='cuda:0')\n",
      "Max absolute difference:\n",
      "O:  tensor(4.9174e-07, device='cuda:0')\n",
      "L:  tensor(1.9073e-06, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "block_dim_x = 32\n",
    "block_dim_y = 16\n",
    "\n",
    "for N_inp, N_out in TEST_DIMS:\n",
    "\n",
    "    Q, K, V, scaling, O_expected, L_expected = get_test_tensors(N_inp, N_out, d)\n",
    "    O_all_smem = torch.zeros(N_out, d, device=\"cuda\").contiguous()\n",
    "    L_all_smem = torch.zeros(N_out, device=\"cuda\")\n",
    "    tpb = (block_dim_x, block_dim_y)\n",
    "    grid = (1,)\n",
    "    flash_attention_numba_all_smem[grid, tpb](Q, K, V, scaling, L_all_smem, O_all_smem,  N_out, N_inp)\n",
    "    torch.cuda.synchronize()\n",
    "    check_close(\n",
    "        O_all_smem, \n",
    "        O_expected,\n",
    "        L_all_smem,\n",
    "        L_expected,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving `m_i`, `l_i`, `O_i` to registers\n",
    "\n",
    "Current shared-memory usage across threads:\n",
    "```\n",
    "Shar = (B_r * d * 4) # Q_i\n",
    "+ (B_c * d * 4) # K_j\n",
    "+ (B_c * d * 4) # V_j\n",
    "+ (B_r * B_c * 4) # S\n",
    "= ~25 KB\n",
    "```\n",
    "\n",
    "Current block-shared accumulators (`m_i`, `l_i`, `O_i`):\n",
    "```\n",
    "Loc = 4 * (B_r + B_r + (B_r * d)) = 8320 B ≈ 8 KB\n",
    "```\n",
    "\n",
    "Total shared usage: **~33 KB** (fine for 1 block/SM).\n",
    "\n",
    "---\n",
    "\n",
    "**Idea:** Move `m_i`, `l_i`, `O_i` to *per-thread* locals to fit in registers.\n",
    "\n",
    "Problem: Full-size per-thread arrays would need\n",
    "\n",
    "```\n",
    "Loc * 32 * 16 ≈ 4 MB > 256 KB register memory per SM\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Optimization:** With tiling `tpb = (32, 16)`:\n",
    "\n",
    "- Each thread handles only  \n",
    "  `d // blockDim.x = 4` columns in `x`  \n",
    "  `B_r // blockDim.y = 1` row in `y`\n",
    "- So per-thread locals can be much smaller:\n",
    "\n",
    "```python\n",
    "l_i = numba.cuda.local.array((1,), inp_dtype)   # 4 B\n",
    "m_i = numba.cuda.local.array((1,), inp_dtype)   # 4 B\n",
    "O_i = numba.cuda.local.array((4,), inp_dtype)   # 16 B\n",
    "```\n",
    "-> Per-thread = 24 B, per block = 24 * 32 * 16 = 12 KB < 256 KB -> avoid register pressure and spills.\n",
    "\n",
    "\n",
    "This is how we set up `flash_attention_numba` below and the cuda version in [`./flash_attention.cu`](./flash_attention.cu)\n",
    "\n",
    "In the performance section we run `./flash_attention_spilling_from_registers.cu` that fits full arrays in local variables, to show the performance decrease by slowing the kernel by ~3×"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we also parallelised the loops in `T_r`, \n",
    "# so that each block processes only one outer loop\n",
    "# and we run a grid of T_r size\n",
    "\n",
    "block_dim_x = 64\n",
    "block_dim_y = 8\n",
    "B_r = 8\n",
    "B_c = 64\n",
    "d_over_dim_x = d // block_dim_x\n",
    "B_r_over_dim_y = B_r // block_dim_y\n",
    "\n",
    "@numba.cuda.jit\n",
    "def flash_attention_numba(Q, K, V, scaling: numba.float32, L, O, N_out, N_inp):\n",
    "    B_c = 16\n",
    "    B_r = 8\n",
    "    T_c = (N_inp + B_c - 1) // B_c\n",
    "    # T_r = (N_out + B_r - 1) // B_r\n",
    "    inp_dtype = K.dtype\n",
    "    tid_x = numba.cuda.threadIdx.x\n",
    "    tid_y = numba.cuda.threadIdx.y\n",
    "    dim_y = numba.cuda.blockDim.y\n",
    "    dim_x = numba.cuda.blockDim.x\n",
    "    block_id_x = numba.cuda.blockIdx.x\n",
    "    \n",
    "\n",
    "    Q_i = numba.cuda.shared.array((B_r, d), inp_dtype)\n",
    "    K_j = numba.cuda.shared.array((B_c, d), inp_dtype)\n",
    "    V_j = numba.cuda.shared.array((B_c, d), inp_dtype)\n",
    "    S = numba.cuda.shared.array((B_r, B_c), inp_dtype)\n",
    "\n",
    "    l_i = numba.cuda.local.array((B_r_over_dim_y,), inp_dtype)\n",
    "    m_i = numba.cuda.local.array((B_r_over_dim_y,), inp_dtype)\n",
    "    O_i = numba.cuda.local.array((B_r_over_dim_y, d_over_dim_x), inp_dtype)\n",
    "\n",
    "                 \n",
    "    for i in range(block_id_x, block_id_x+1):\n",
    "        for ii in range(tid_y, B_r, dim_y):\n",
    "            for dd in range(tid_x, d, dim_x):\n",
    "                Q_i[ii, dd] = Q[ii + i * B_r, dd]\n",
    "\n",
    "        for ii in range(B_r_over_dim_y):\n",
    "            for dd in range(d_over_dim_x):\n",
    "                O_i[ii, dd] = 0\n",
    "            l_i[ii] = 0\n",
    "            m_i[ii] = -math.inf\n",
    "        \n",
    "        numba.cuda.syncthreads()\n",
    "        for j in range(T_c):\n",
    "            for jj in range(tid_y, B_c, dim_y):\n",
    "                for dd in range(tid_x, d, dim_x):\n",
    "                    K_j[jj, dd] = K[jj + j * B_c, dd]\n",
    "                    V_j[jj, dd] = V[jj + j * B_c, dd]\n",
    "\n",
    "            # S[ii][jj] = scaling * (Q_i @ K_j.T)\n",
    "            numba.cuda.syncthreads()\n",
    "            for ii in range(tid_x, B_r, dim_x):\n",
    "                for jj in range(tid_y, B_c, dim_y):\n",
    "                    S_ij = 0\n",
    "                    for dd in range(d):\n",
    "                        S_ij += Q_i[ii, dd] * K_j[jj, dd]\n",
    "                    S_ij = scaling * S_ij\n",
    "                    S[ii][jj] = S_ij\n",
    "\n",
    "            numba.cuda.syncthreads()\n",
    "            for ii in range(B_r_over_dim_y):\n",
    "                m = m_i[ii]\n",
    "                last_m = m\n",
    "                for jj in range(B_c):\n",
    "                    m = max(m, S[ii * dim_y + tid_y][jj])\n",
    "                m_i[ii] = m\n",
    "                l = numba.float32(math.exp(last_m - m)) * l_i[ii]\n",
    "\n",
    "                for dd in range(d_over_dim_x):\n",
    "                    O_i[ii, dd] *= numba.float32(math.exp(last_m - m))\n",
    "                for jj in range(B_c):\n",
    "                    P_ij = numba.float32(math.exp(S[ii * dim_y + tid_y][jj] - m))\n",
    "                    l += P_ij\n",
    "                    for dd in range(d_over_dim_x):\n",
    "                        O_i[ii, dd] += P_ij * V_j[jj, dd * dim_x + tid_x]\n",
    "                l_i[ii] = l\n",
    "                \n",
    "        for ii in range(B_r_over_dim_y):  \n",
    "            for dd in range(d_over_dim_x):\n",
    "                O[ii * dim_y + tid_y + i * B_r, dd * dim_x + tid_x] = O_i[ii, dd] / l_i[ii]\n",
    "            L[ii * dim_y + tid_y + i * B_r] = m_i[ii] + numba.float32(math.log(l_i[ii]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 4 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max absolute difference:\n",
      "O:  tensor(3.5763e-07, device='cuda:0')\n",
      "L:  tensor(4.7684e-07, device='cuda:0')\n",
      "Max absolute difference:\n",
      "O:  tensor(3.2783e-07, device='cuda:0')\n",
      "L:  tensor(9.5367e-07, device='cuda:0')\n",
      "Max absolute difference:\n",
      "O:  tensor(4.0233e-07, device='cuda:0')\n",
      "L:  tensor(1.4305e-06, device='cuda:0')\n",
      "Max absolute difference:\n",
      "O:  tensor(3.9488e-07, device='cuda:0')\n",
      "L:  tensor(1.9073e-06, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 8 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/opt/conda/lib/python3.12/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 64 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "for N_inp, N_out in TEST_DIMS:\n",
    "    Q, K, V, scaling, O_expected, L_expected = get_test_tensors(N_inp, N_out, d)\n",
    "\n",
    "    O_all_smem = torch.zeros(N_out, d, device=\"cuda\").contiguous()\n",
    "    L_all_smem = torch.zeros(N_out, device=\"cuda\")\n",
    "    tpb = (block_dim_x, block_dim_y)\n",
    "    T_r = (N_out + B_r - 1) // B_r\n",
    "    grid = (T_r,)\n",
    "    flash_attention_numba[grid, tpb](Q, K, V, scaling, L_all_smem, O_all_smem,  N_out, N_inp)\n",
    "    check_close(\n",
    "        O_all_smem, \n",
    "        O_expected,\n",
    "        L_all_smem,\n",
    "        L_expected,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuda\n",
    "\n",
    "### flash_attention_numba in Cuda\n",
    "\n",
    "See [flash_attention.cu](./flash_attention.cu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2073: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "fname = \"flash_attention\"\n",
    "module = get_loaded_cuda_module(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max absolute difference:\n",
      "O:  tensor(4.4703e-07, device='cuda:0')\n",
      "L:  tensor(4.7684e-07, device='cuda:0')\n",
      "Max absolute difference:\n",
      "O:  tensor(4.1723e-07, device='cuda:0')\n",
      "L:  tensor(4.7684e-07, device='cuda:0')\n",
      "Max absolute difference:\n",
      "O:  tensor(5.6624e-07, device='cuda:0')\n",
      "L:  tensor(9.5367e-07, device='cuda:0')\n",
      "Max absolute difference:\n",
      "O:  tensor(3.7253e-07, device='cuda:0')\n",
      "L:  tensor(1.9073e-06, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for N_inp, N_out in TEST_DIMS:\n",
    "    Q, K, V, _, O_expected, L_expected = get_test_tensors(N_inp, N_out, d)\n",
    "    O_move_registers, L_move_registers = getattr(module, fname)(Q, K, V)\n",
    "    check_close(\n",
    "        O_move_registers, \n",
    "        O_expected,\n",
    "        L_move_registers,\n",
    "        L_expected,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import triton\n",
    "# import triton.language as tl\n",
    "# import math\n",
    "\n",
    "\n",
    "# @triton.jit\n",
    "# def flash_attn_triton(\n",
    "#     Q, K, V, O, L,\n",
    "#     M: tl.constexpr, N: tl.constexpr, D: tl.constexpr,\n",
    "#     stride_qm, stride_qd,\n",
    "#     stride_km, stride_kd,\n",
    "#     stride_vm, stride_vd,\n",
    "#     stride_om, stride_od,\n",
    "#     scaling: tl.constexpr,\n",
    "#     BLOCK_M: tl.constexpr,      # >=16\n",
    "#     BLOCK_N: tl.constexpr,      # >=16\n",
    "#     BLOCK_D_TILE: tl.constexpr, # >=16 (for tl.dot inner dim)\n",
    "# ):\n",
    "#     pid_m = tl.program_id(0)\n",
    "#     offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
    "\n",
    "#     # per-program accumulators\n",
    "#     m_i = tl.full((BLOCK_M,), -float(\"inf\"), dtype=tl.float32)\n",
    "#     l_i = tl.zeros((BLOCK_M,), dtype=tl.float32)\n",
    "#     acc = tl.zeros((BLOCK_M, D), dtype=tl.float32)  # D small in your use-case (128)\n",
    "\n",
    "#     # Precompute Q chunks for reuse across K tiles:\n",
    "#     # We'll load Q in D-chunks on demand inside the D loop below.\n",
    "\n",
    "#     # Sweep over K/V tiles (columns)\n",
    "#     for start_n in range(0, N, BLOCK_N):\n",
    "#         offs_n = start_n + tl.arange(0, BLOCK_N)\n",
    "#         # Build scores for this (BLOCK_M x BLOCK_N) tile by accumulating across D chunks\n",
    "#         scores_tile = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n",
    "\n",
    "#         # accumulate scores across D in tiles so each tl.dot sees K >= 16\n",
    "#         for start_d in range(0, D, BLOCK_D_TILE):\n",
    "#             offs_d = start_d + tl.arange(0, BLOCK_D_TILE)\n",
    "\n",
    "#             # load Q chunk: [BLOCK_M, D_chunk]\n",
    "#             q_ptrs = Q + (offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qd)\n",
    "#             q_mask = (offs_m[:, None] < M) & (offs_d[None, :] < D)\n",
    "#             q_chunk = tl.load(q_ptrs, mask=q_mask, other=0.0).to(tl.float32)\n",
    "\n",
    "#             # load K chunk: [BLOCK_N, D_chunk]\n",
    "#             k_ptrs = K + (offs_n[:, None] * stride_km + offs_d[None, :] * stride_kd)\n",
    "#             k_mask = (offs_n[:, None] < N) & (offs_d[None, :] < D)\n",
    "#             k_chunk = tl.load(k_ptrs, mask=k_mask, other=0.0).to(tl.float32)\n",
    "\n",
    "#             # partial scores for this d-chunk: [BLOCK_M, BLOCK_N]\n",
    "#             # tl.dot(q_chunk, k_chunk.T) requires D_chunk >= 16 for tensorcore lowering\n",
    "#             partial = tl.dot(q_chunk, tl.trans(k_chunk))\n",
    "#             scores_tile = scores_tile + partial * scaling\n",
    "\n",
    "#         # mask invalid columns (columns beyond N)\n",
    "#         col_idxs = start_n + tl.arange(0, BLOCK_N)\n",
    "#         col_mask = col_idxs[None, :] < N\n",
    "#         # masked max (invalid columns must not influence max)\n",
    "#         scores_masked = tl.where(col_mask, scores_tile, tl.full(scores_tile.shape, -float(\"inf\"), dtype=tl.float32))\n",
    "#         # numerically stable online-softmax update for this tile\n",
    "#         m_ij = tl.max(scores_masked, axis=1)            # per-row max for this tile\n",
    "#         m_new = tl.maximum(m_i, m_ij)\n",
    "#         # exponent factor for old -> new max\n",
    "#         alpha = tl.exp(m_i - m_new)                      # shape [BLOCK_M]\n",
    "#         # compute probabilities p = exp(scores_tile - m_new[:,None]) with mask\n",
    "#         p = tl.where(col_mask, tl.exp(scores_tile - m_new[:, None]), tl.zeros_like(scores_tile))\n",
    "\n",
    "#         # update l_i (scalar per row) and acc (vector per row over D)\n",
    "#         # We need contribution = p @ V_tile (shape [BLOCK_M, D]), we compute it by D-chunking V.\n",
    "#         contrib = tl.zeros((BLOCK_M, D), dtype=tl.float32)\n",
    "\n",
    "#         for start_d in range(0, D, BLOCK_D_TILE):\n",
    "#             offs_d = start_d + tl.arange(0, BLOCK_D_TILE)\n",
    "\n",
    "#             # load V chunk: [BLOCK_N, D_chunk]\n",
    "#             v_ptrs = V + (offs_n[:, None] * stride_vm + offs_d[None, :] * stride_vd)\n",
    "#             v_mask = (offs_n[:, None] < N) & (offs_d[None, :] < D)\n",
    "#             v_chunk = tl.load(v_ptrs, mask=v_mask, other=0.0).to(tl.float32)\n",
    "\n",
    "#             # p @ v_chunk -> [BLOCK_M, D_chunk]\n",
    "#             contrib_chunk = tl.dot(p, v_chunk)\n",
    "\n",
    "#             # write contrib_chunk into the right slice of contrib (per-D)\n",
    "#             # We can't slice assign in Triton, so build a zero buffer and add shifted\n",
    "#             # we'll accumulate into contrib by adding at correct columns via masking and pointer maths\n",
    "#             # Prepare pointers into a fake global buffer: use O as temporary workspace ptrs (safe as we don't rely on O until the end)\n",
    "#             # But simpler: accumulate into contrib full matrix and then later use it.\n",
    "#             # Build a tile-sized mask and place chunk at [ :, start_d:start_d+D_chunk ]\n",
    "#             # We'll build a small tensor temp_full of shape (BLOCK_M, BLOCK_D_TILE) and then write into contrib via elementwise add.\n",
    "#             # Instead of complex pointer arithmetic, we do a python-level placement emulation:\n",
    "#             # create a view on contrib for the slice and add (Triton currently allows direct arithmetic with slices using offsets)\n",
    "#             # compute column global offsets for these chunk columns\n",
    "#             col_offset = start_d\n",
    "#             # We'll do elementwise add by computing an index matrix and mask store:\n",
    "#             offs_d_global = col_offset + tl.arange(0, BLOCK_D_TILE)   # shape [BLOCK_D_TILE]\n",
    "#             mask_d = offs_d_global[None, :] < D\n",
    "#             # compute pointers into a pseudo buffer using O as base (but we want to avoid clobbering O mid-kernel)\n",
    "#             # Simpler: directly accumulate into contrib by constructing a broadcast and using `tl.store` into a local slice.\n",
    "#             # Triton does not support direct writes into a local Python array slice; instead accumulate by adding into contrib using broadcasting\n",
    "#             # We'll rebuild contrib with addition over the slice:\n",
    "#             # Build zeros buffer of same shape as contrib and add contribution in place by combining masks:\n",
    "#             # Efficient and simpler: use `tl.where` to add contrib_chunk into the appropriate columns\n",
    "#             # Build an index matrix for columns: col_idx_mat [1, D] where positions in this chunk are True\n",
    "#             col_idx_mat = offs_d_global[None, :]  # shape [1, BLOCK_D_TILE]\n",
    "#             # Create boolean mask of size (1, D) for these columns; then broadcast to (BLOCK_M, D)\n",
    "#             # But Triton doesn't allow creating a mask of length D easily. Simpler: implement contrib as accumulation per chunk into acc directly:\n",
    "#             # acc[:, start_d:start_d+D_chunk] += contrib_chunk\n",
    "#             # We can store the chunk to global memory O temporarily and then later read; that's messy.\n",
    "#             # Given D is small (128) we can instead hold contrib as a flat contiguous buffer and write chunk via pointer math:\n",
    "#             # We'll compute pointer into a per-program scratch area in global memory represented by O + big offset.\n",
    "#             # To avoid complexity, we will construct a temporary full-width tile with zeros and place contrib_chunk into columns [start_d:start_d+D_chunk] using masks:\n",
    "#             # Create temp_tile = zeros((BLOCK_M, D))\n",
    "#             temp_tile = tl.zeros((BLOCK_M, D), dtype=tl.float32)\n",
    "#             # Build pointers for temp tile positions (this is only conceptual; we cannot create a huge temp easily)\n",
    "#             # Instead we use an element-wise construction: for each column in BLOCK_D_TILE, place contrib_chunk column into the corresponding global column position\n",
    "#             for k_col in range(0, BLOCK_D_TILE):\n",
    "#                 col = start_d + k_col\n",
    "#                 valid_col = col < D\n",
    "#                 if valid_col:\n",
    "#                     # add contrib_chunk[:, k_col] into contrib[:, col]\n",
    "#                     # Build a small vector and perform elementwise addition through reassigning contrib via tl.where trick:\n",
    "#                     # get col values of contrib (current)\n",
    "#                     # we can't index contrib[:, col] as an lvalue; instead we do:\n",
    "#                     # contrib = contrib + (mask_column * expand(contrib_chunk[:, k_col]))\n",
    "#                     # mask_column shape: (1,D) with True at position col. We'll build mask by comparing an arange to col.\n",
    "#                     cols_all = tl.arange(0, D)\n",
    "#                     mask_col_all = (cols_all[None, :] == col)\n",
    "#                     # broadcast contrib_chunk column\n",
    "#                     to_add = contrib_chunk[:, k_col][:, None] * mask_col_all\n",
    "#                     contrib = contrib + to_add\n",
    "#                 # else ignore\n",
    "\n",
    "#         # After contrib computed (shape [BLOCK_M, D]) and p ready\n",
    "#         # Update acc and l_i\n",
    "#         l_new = alpha * l_i + tl.sum(p, axis=1)\n",
    "#         acc = (alpha[:, None] * acc) + contrib\n",
    "#         m_i = m_new\n",
    "#         l_i = l_new\n",
    "\n",
    "#     # Write outputs O (normalize) and L\n",
    "#     out = acc / l_i[:, None]\n",
    "#     # store O\n",
    "#     offs_d_full = tl.arange(0, D)\n",
    "#     o_ptrs = O + (offs_m[:, None] * stride_om + offs_d_full[None, :] * stride_od)\n",
    "#     mask_o = (offs_m[:, None] < M) & (offs_d_full[None, :] < D)\n",
    "#     tl.store(o_ptrs, out, mask=mask_o)\n",
    "#     # store L\n",
    "#     tl.store(L + offs_m, m_i + tl.log(l_i), mask=offs_m < M)\n",
    "\n",
    "\n",
    "# def flash_attention_triton(Q, K, V):\n",
    "#     \"\"\"\n",
    "#     Q, K, V: torch tensors [N_out, d], [N_inp, d], [N_inp, d]\n",
    "#              (fp16/bf16/fp32). Returns O [N_out, d], L [N_out].\n",
    "#     scaling: float (e.g., 1/sqrt(d))\n",
    "#     \"\"\"\n",
    "#     assert Q.is_cuda and K.is_cuda and V.is_cuda\n",
    "#     assert Q.shape[1] == K.shape[1] == V.shape[1]\n",
    "#     M, D = Q.shape\n",
    "#     N = K.shape[0]\n",
    "#     scaling = 1./math.sqrt(Q.shape[1])\n",
    "\n",
    "#     # output buffers\n",
    "#     O = torch.empty_like(Q, dtype=torch.float32)  # accum in fp32; cast later if you want\n",
    "#     L = torch.empty(M, device=Q.device, dtype=torch.float32)\n",
    "\n",
    "#     # tile sizes (match your numba defaults if you like)\n",
    "#     BLOCK_M = 16    # B_r\n",
    "#     BLOCK_N = 16   # B_c\n",
    "#     BLOCK_D = D    # set to head dim; Triton specializes per call\n",
    "\n",
    "#     # launch grid: one program per BLOCK_M rows\n",
    "#     grid = (triton.cdiv(M, BLOCK_M),)\n",
    "\n",
    "#     flash_attn_triton[grid](\n",
    "#         Q, K, V, O, L,\n",
    "#         M, N, D,\n",
    "#         Q.stride(0), Q.stride(1),\n",
    "#         K.stride(0), K.stride(1),\n",
    "#         V.stride(0), V.stride(1),\n",
    "#         O.stride(0), O.stride(1),\n",
    "#         float(scaling),\n",
    "#         BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_D_TILE=BLOCK_D,\n",
    "#         num_warps=4,  # good starting point; tune as needed\n",
    "#         num_stages=2,\n",
    "#     )\n",
    "\n",
    "#     return O, L\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for N_inp, N_out in TEST_DIMS:\n",
    "#     Q, K, V, scaling, O_expected, L_expected = get_test_tensors(N_inp, N_out, d)\n",
    "\n",
    "#     O_triton = torch.zeros(N_out, d, device=\"cuda\").contiguous()\n",
    "#     L_triton = torch.zeros(N_out, device=\"cuda\")\n",
    "  \n",
    "#     flash_attention_triton(Q, K, V)\n",
    "#     check_close(\n",
    "#         O_triton, \n",
    "#         O_expected,\n",
    "#         L_triton,\n",
    "#         L_expected,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance \n",
    "\n",
    "Run timeit on different dimensions. \n",
    "\n",
    "- Recall that we build the kernel for `d=128` and design it so that it computes the full attention in a single block.\n",
    "\n",
    "- For matrices with small `N_out` this implementation is comparable with `scaled_dot_product_attention` faster backends.\n",
    "\n",
    "- But by making N larger this implementation slows down dramatically it only uses a single block.\n",
    "\n",
    "- Note that the register splilling version is always much slower than `scaled_dot_product_attention`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load registers spilling version\n",
    "\n",
    "This is the version loading full arrays as local variables in threads, which leads to spilling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2073: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max absolute difference:\n",
      "O:  tensor(3.7253e-07, device='cuda:0')\n",
      "L:  tensor(1.9073e-06, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "fname_spill_from_registers = \"flash_attention_spilling_from_registers\"\n",
    "module_spilling_from_registers = get_loaded_cuda_module(fname_spill_from_registers)\n",
    "O_cuda_spilling, L_cuda_spilling = getattr(module_spilling_from_registers, fname_spill_from_registers)(Q, K, V)\n",
    "check_close(O_cuda_spilling, O_expected, L_cuda_spilling, L_expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timeit: sdpa backends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Explicit softmax(Q @ K.T * scaling) @ V\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334 μs ± 521 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "\n",
      "Backend: None\n",
      "379 μs ± 64.6 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "\n",
      "Backend: EFFICIENT_ATTENTION\n",
      "589 μs ± 873 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "\n",
      "Backend: FLASH_ATTENTION (f16)\n",
      "111 μs ± 322 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "\n",
      "Backend: CUDNN_ATTENTION (f16)\n",
      "162 μs ± 27.8 μs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# Benchmark first the different backends for scaled_dot_product_attention\n",
    "# For small tensors\n",
    "from torch.nn.attention import sdpa_kernel, SDPBackend\n",
    "\n",
    "Q, K, V, _, _, _ = get_test_tensors(N_inp=2048, N_out=2048, d=128)\n",
    "Qhalf, Khalf, Vhalf = Q.to(torch.float16), K.to(torch.float16), V.to(torch.float16)\n",
    "\n",
    "def run_sdpa_with_backend(Q, K, V, backend):\n",
    "    with sdpa_kernel(backends=backend):\n",
    "        torch.nn.functional.scaled_dot_product_attention(\n",
    "            Q.unsqueeze(0).unsqueeze(0), \n",
    "            K.unsqueeze(0).unsqueeze(0), \n",
    "            V.unsqueeze(0).unsqueeze(0),\n",
    "        )\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "print(\"\\nExplicit softmax(Q @ K.T * scaling) @ V\")\n",
    "%timeit torch.softmax(Q @ K.T * scaling, dim=-1) @ V; torch.cuda.synchronize()\n",
    "print(\"\\nBackend: None\")\n",
    "%timeit torch.nn.functional.scaled_dot_product_attention(Q, K, V)\n",
    "print(\"\\nBackend: EFFICIENT_ATTENTION\")\n",
    "%timeit run_sdpa_with_backend(Q, K, V, backend=SDPBackend.EFFICIENT_ATTENTION)\n",
    "print(\"\\nBackend: FLASH_ATTENTION (f16)\")\n",
    "%timeit run_sdpa_with_backend(Qhalf, Khalf, Vhalf, backend=SDPBackend.FLASH_ATTENTION)\n",
    "print(\"\\nBackend: CUDNN_ATTENTION (f16)\")\n",
    "%timeit run_sdpa_with_backend(Qhalf, Khalf, Vhalf, backend=SDPBackend.CUDNN_ATTENTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timeit: sdpa vs custom kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "**********\n",
      "Dimensions: N_out=32, N_inp=32, d=128\n",
      "\n",
      "- Torch scaled_dot_product_attention\n",
      "104 μs ± 868 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "\n",
      "- Custom Flash Attention\n",
      "39.3 μs ± 277 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "\n",
      "- Custom Flash Attention: spill from registers\n",
      "313 μs ± 659 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "\n",
      "\n",
      "**********\n",
      "Dimensions: N_out=64, N_inp=128, d=128\n",
      "\n",
      "- Torch scaled_dot_product_attention\n",
      "106 μs ± 826 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "\n",
      "- Custom Flash Attention\n",
      "76.5 μs ± 361 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "\n",
      "- Custom Flash Attention: spill from registers\n",
      "2.26 ms ± 1.3 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "\n",
      "**********\n",
      "Dimensions: N_out=512, N_inp=512, d=128\n",
      "\n",
      "- Torch scaled_dot_product_attention\n",
      "112 μs ± 731 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "\n",
      "- Custom Flash Attention\n",
      "229 μs ± 735 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "\n",
      "- Custom Flash Attention: spill from registers\n",
      "70.8 ms ± 8.69 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "\n",
      "**********\n",
      "Dimensions: N_out=512, N_inp=1024, d=128\n",
      "\n",
      "- Torch scaled_dot_product_attention\n",
      "112 μs ± 429 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "\n",
      "- Custom Flash Attention\n",
      "431 μs ± 560 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "\n",
      "- Custom Flash Attention: spill from registers\n",
      "141 ms ± 10.1 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Check now run against \n",
    "for N_inp, N_out in TEST_DIMS:\n",
    "    Q, K, V, _, _, _ = get_test_tensors(N_inp, N_out, d)\n",
    "    print(f\"\\n\\n**********\\nDimensions: {N_out=}, {N_inp=}, {d=}\")\n",
    "    torch.cuda.synchronize()\n",
    "    print(\"\\n- Torch scaled_dot_product_attention\")\n",
    "    %timeit torch.nn.functional.scaled_dot_product_attention(Q, K, V); torch.cuda.synchronize()\n",
    "    print(\"\\n- Custom Flash Attention\")\n",
    "    %timeit getattr(module, fname)(Q, K, V); torch.cuda.synchronize()\n",
    "    print(\"\\n- Custom Flash Attention: spill from registers\")\n",
    "    %timeit getattr(module_spilling_from_registers, fname_spill_from_registers)(Q, K, V); torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profile\n",
    "\n",
    "We can compare the performance of the kernel with and without register spilling as follows:\n",
    "```\n",
    "# Get ptx files or use https://godbolt.org/\n",
    "nvcc -ptx flash_attention.cu -o flash_attention.ptx\n",
    "nvcc -ptx flash_attention_spilling_from_registers.cu -o flash_attention_spilling_from_registers.ptx\n",
    "\n",
    "# Get ncu metrics\n",
    "nvcc -O3 -o test_attention main.cu flash_attention.cu flash_attention_spilling_from_registers.cu\n",
    "ncu ./test_attention\n",
    "```\n",
    "\n",
    "#### PTX comparison\n",
    "\n",
    "For the [spilling kernel](./flash_attention_spilling_from_registers.cu) we see \n",
    "```ptx\n",
    ".local .align 16 .b8 \t__local_depot0[8320];\n",
    "```\n",
    "where 8320 B is exactly the allocation for\n",
    "```cu\n",
    "float l_i[B_r];\n",
    "float m_i[B_r];\n",
    "float O_i[B_r][d];\n",
    "```\n",
    "\n",
    "\n",
    "#### Nsight Compute Comparison\n",
    "\n",
    "| **Metric** | **Spilling Kernel (`flash_attention_spilling_from_registers_k`)** | **Non-Spilling Kernel (`flash_attention_k`)** | **Difference** |\n",
    "|------------|------------------------------------------------------------------|-----------------------------------------------|---------------------------------|\n",
    "| **Duration** | **13.02 ms** | **2.10 ms** | Spilling kernel is ~6× slower |\n",
    "| **Compute (SM) Throughput** | **0.26%** | **1.20%** | 5× higher compute utilization in non-spilling kernel |\n",
    "| **L2 Cache Throughput** | **1.71%** | **0.56%** | Spilling kernel hits L2 more |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuda-Python\n",
    "\n",
    "`conda install conda-forge::cuda-python`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "\u0000\n",
      "0\n",
      "0 <CUfunction 0x55669f57f960>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap_external>:1301: FutureWarning: The cuda.cuda module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.driver module instead.\n",
      "<frozen importlib._bootstrap_external>:1301: FutureWarning: The cuda.nvrtc module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.nvrtc module instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([93899257721792, 93899257721800, 93899257721808, 93899257721816,\n",
       "        93899257721824, 93899250177280, 93899249702528, 93899249702532,\n",
       "        93899249702536], dtype=torch.uint64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create program\n",
    "from cuda import cuda, nvrtc\n",
    "\n",
    "cuda_src_path = f\"./flash_attention.cu\"\n",
    "cuda_src = Path(cuda_src_path).read_text()\n",
    "\n",
    "N_inp = 32\n",
    "N_out = 32\n",
    "d = 128\n",
    "B_r, B_c = 16, 16\n",
    "T_r = (N_out + B_r -1) // B_r\n",
    "T_c = (N_inp + B_r -1) // B_c\n",
    "Q, K, V, scale_factor, O_expected, L_expected = get_test_tensors(N_inp, N_out, d)\n",
    "\n",
    "err, prog = nvrtc.nvrtcCreateProgram(str.encode(cuda_src), b\"flash_attention.cu\", 0, [], [])\n",
    "\n",
    "# Compile program\n",
    "min, maj = torch.cuda.get_device_capability()\n",
    "opts = [\n",
    "    f\"--gpu-architecture=compute_{min}{maj}\".encode(), \n",
    "    \"--device-as-default-execution-space\".encode(),\n",
    "    \"--std=c++14\".encode()]\n",
    "err, = nvrtc.nvrtcCompileProgram(prog, len(opts), opts)\n",
    "\n",
    "print(err)\n",
    "\n",
    "# Get PTX from compilation\n",
    "err, ptxSize = nvrtc.nvrtcGetPTXSize(prog)\n",
    "ptx = b\" \" * ptxSize\n",
    "err, = nvrtc.nvrtcGetPTX(prog, ptx)\n",
    "print(err)\n",
    "\n",
    "err, logSize = nvrtc.nvrtcGetProgramLogSize(prog)\n",
    "log = b\" \" * logSize\n",
    "err, = nvrtc.nvrtcGetProgramLog(prog, log)\n",
    "print(log.decode())\n",
    "# print(ptx.decode())\n",
    "\n",
    "# Load PTX as module data and retrieve function\n",
    "err, module = cuda.cuModuleLoadData(ptx)\n",
    "print(err)\n",
    "err, kernel = cuda.cuModuleGetFunction(module, b\"flash_attention_k\")\n",
    "print(err, kernel)\n",
    "\n",
    "# Allocate tensors\n",
    "# S3 = torch.zeros(N_out, N_out, device=\"cuda\")\n",
    "O_cuda_py = torch.zeros(N_out, d, device=\"cuda\")\n",
    "L_cuda_py = torch.zeros(N_out, device=\"cuda\")\n",
    "\n",
    "# To quote the official tutorial: (https://nvidia.github.io/cuda-python/overview.html)\n",
    "# The following code example is not intuitive\n",
    "# Subject to change in a future release\n",
    "\n",
    "int_args = torch.tensor([0, T_r, T_c], dtype=torch.int32)\n",
    "float_args = torch.tensor([scale_factor], dtype=torch.float32)\n",
    "ptr_args = torch.tensor([i.data_ptr() for i in (O_cuda_py, L_cuda_py, Q, K, V)], dtype=torch.uint64)\n",
    "\n",
    "args = torch.tensor([\n",
    "    *(i.data_ptr() for i in ptr_args),\n",
    "    *(i.data_ptr() for i in float_args),\n",
    "    *(i.data_ptr() for i in int_args)], dtype=torch.uint64)\n",
    "\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2352, device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fn():\n",
    "    err = cuda.cuLaunchKernel(\n",
    "        kernel,\n",
    "        T_r,  # grid x dim\n",
    "        1,  # grid y dim\n",
    "        1,  # grid z dim\n",
    "        16,  # block x dim\n",
    "        16,  # block y dim\n",
    "        1,  # block z dim\n",
    "        0,  # dynamic shared memory\n",
    "        torch.cuda.current_stream().stream_id,  # stream\n",
    "        args.data_ptr(),  # kernel arguments\n",
    "        0,  # extra (ignore)\n",
    "    )\n",
    "\n",
    "fn()\n",
    "\n",
    "(O_cuda_py - O_expected).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "**********\n",
      "Dimensions: N_out=32, N_inp=32, d=128\n",
      "\n",
      "- Custom Flash Attention: Cuda-python\n",
      "35.3 μs ± 228 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n\\n**********\\nDimensions: {N_out=}, {N_inp=}, {d=}\")\n",
    "torch.cuda.synchronize()\n",
    "print(\"\\n- Custom Flash Attention: Cuda-python\")\n",
    "%timeit fn(); torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thunder\n",
    "\n",
    "[Installation guide](https://lightning.ai/docs/thunder/latest/fundamentals/installation.html)\n",
    "\n",
    "We use thunder here to include our custom kernel as an option within torch's own `scaled_dot_product_attention`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/thunder/executors/transformer_engine_v2ex.py:25: UserWarning: transformer_engine module not found!\n",
      "  warnings.warn(\"transformer_engine module not found!\")\n"
     ]
    }
   ],
   "source": [
    "import thunder\n",
    "\n",
    "attn_ex = thunder.extend.OperatorExecutor('attn_ex', version=0.01)\n",
    "thunder.add_default_executor(attn_ex)\n",
    "\n",
    "# [attn_ex, attn_ex, sdpa, nvfuser]\n",
    "\n",
    "def my_attn_impl(query, key, value, scale):\n",
    "    n_out, d = query.shape\n",
    "\n",
    "    # S3 = torch.zeros(N_out, N_out, device=\"cuda\")\n",
    "    O3 = torch.zeros(N_out, d, device=\"cuda\")\n",
    "    L3 = torch.zeros(N_out, device=\"cuda\")\n",
    "\n",
    "    B_c = 16\n",
    "    B_r = 16\n",
    "    T_c = (N_inp + B_c - 1) // B_c\n",
    "    T_r = (N_out + B_r - 1) // B_r\n",
    "\n",
    "    int_args = torch.tensor([N_out, T_r, T_c], dtype=torch.int32)\n",
    "    float_args = torch.tensor([scale_factor], dtype=torch.float32)\n",
    "    ptr_args = torch.tensor([i.data_ptr() for i in (O3, L3, key, query, value)], dtype=torch.uint64)\n",
    "\n",
    "    args = torch.tensor([\n",
    "        *(i.data_ptr() for i in ptr_args),\n",
    "        *(i.data_ptr() for i in float_args),\n",
    "        *(i.data_ptr() for i in int_args)], dtype=torch.uint64\n",
    "    )\n",
    "\n",
    "    err, _ = cuda.cuLaunchKernel(\n",
    "        kernel,\n",
    "        T_r,  # grid x dim\n",
    "        1,  # grid y dim\n",
    "        1,  # grid z dim\n",
    "        32, # block x dim\n",
    "        32, # block y dim\n",
    "        1,  # block z dim\n",
    "        0,  # dynamic shared memory\n",
    "        torch.cuda.current_stream().stream_id,  # stream\n",
    "        args.data_ptr(),  # kernel arguments\n",
    "        0,  # extra (ignore)\n",
    "    )\n",
    "    assert err == cuda.CUresult.CUDA_SUCCESS, err\n",
    "    return O3, L3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Register our implementation as an operator\n",
    "def my_attn_meta(query, key, value, scale):\n",
    "    return thunder.TensorProxy(like=query), thunder.TensorProxy(like=query, shape=(query.shape[:-1],))\n",
    "\n",
    "my_attn = attn_ex.register_operator('my_attn', meta=my_attn_meta, fn=my_attn_impl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_attn_checker(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False, scale=None):\n",
    "    if attn_mask is not None or dropout_p == 0.0 or is_causal:\n",
    "        return False\n",
    "    if len(query.shape) > 2:\n",
    "            return (query.device.device_type == thunder.devices.DeviceType.CUDA and\n",
    "                key.device == query.device and\n",
    "                value.device == query.device)\n",
    "    return False\n",
    "\n",
    "def my_attn_transform(query, key, value, attn_masks=None, dropout_p=0.0, is_causal=False, scale=None):\n",
    "    if scale is None:\n",
    "        scale = query.size(-1) ** -0.5\n",
    "    out = my_attn(query, key, value, scale)\n",
    "    return out[0]\n",
    "\n",
    "attn_ex.register_implementation(thunder.torch.scaled_dot_product_attention, checker=my_attn_checker,\n",
    "                                  execution_transform=my_attn_transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.6624e-07)\n",
      "# Constructed by Unwrap the actual return value\n",
      "import torch\n",
      "import torch.nn.functional\n",
      "from thunder.executors.torchex import no_autocast\n",
      "\n",
      "@torch.no_grad()\n",
      "@no_autocast\n",
      "def computation(query, key, value):\n",
      "  # query: \"cuda:0 f32[32, 128]\"\n",
      "  # key: \"cuda:0 f32[32, 128]\"\n",
      "  # value: \"cuda:0 f32[32, 128]\"\n",
      "\n",
      "  # /tmp/ipykernel_17219/2358261693.py:2: \t        return torch.nn.functional.scaled_dot_product_attention(query, key, value, is_causal=False)\n",
      "  t41 = torch.nn.functional.scaled_dot_product_attention(query, key, value, None, 0.0, False, scale=None)  # t41: \"cuda:0 f32[32, 128]\"\n",
      "    # t41 = ltorch.scaled_dot_product_attention(query, key, value, None, 0.0, False, scale=None)  # t41: \"cuda:0 f32[32, 128]\"\n",
      "      # t28 = ltorch.mul(query, 0.29730177875068026)  # t28: \"cuda:0 f32[32, 128]\"\n",
      "        # t28 = prims.mul(query, 0.29730177875068026)  # t28: \"cuda:0 f32[32, 128]\"\n",
      "      # t29 = ltorch.transpose(key, -2, -1)  # t29: \"cuda:0 f32[128, 32]\"\n",
      "        # t29 = prims.transpose(key, (1, 0))  # t29: \"cuda:0 f32[128, 32]\"\n",
      "      # t30 = ltorch.mul(t29, 0.29730177875068026)  # t30: \"cuda:0 f32[128, 32]\"\n",
      "        # t30 = prims.mul(t29, 0.29730177875068026)  # t30: \"cuda:0 f32[128, 32]\"\n",
      "      # t31 = ltorch.matmul(t28, t30)  # t31: \"cuda:0 f32[32, 32]\"\n",
      "        # t31 = prims.matmul(t28, t30)  # t31: \"cuda:0 f32[32, 32]\"\n",
      "      # t40 = ltorch._softmax(t31, -1, dtype=None)  # t40: \"cuda:0 f32[32, 32]\"\n",
      "        # t33 = ltorch.amax(t31, -1, True)  # t33: \"cuda:0 f32[32, 1]\"\n",
      "          # t32 = prims.amax(t31, (1,))  # t32: \"cuda:0 f32[32]\"\n",
      "          # t33 = prims.broadcast_in_dim(t32, [32, 1], [0])  # t33: \"cuda:0 f32[32, 1]\"\n",
      "        # t35 = ltorch.sub(t31, t33, alpha=1)  # t35: \"cuda:0 f32[32, 32]\"\n",
      "          # t34 = prims.broadcast_in_dim(t33, (32, 32), (0, 1))  # t34: \"cuda:0 f32[32, 32]\"\n",
      "          # t35 = prims.sub(t31, t34)  # t35: \"cuda:0 f32[32, 32]\"\n",
      "        # t36 = ltorch.exp(t35)  # t36: \"cuda:0 f32[32, 32]\"\n",
      "          # t36 = prims.exp(t35)  # t36: \"cuda:0 f32[32, 32]\"\n",
      "        # t38 = ltorch.sum(t36, -1, True, dtype=None)  # t38: \"cuda:0 f32[32, 1]\"\n",
      "          # t37 = prims.sum(t36, (1,))  # t37: \"cuda:0 f32[32]\"\n",
      "          # t38 = prims.broadcast_in_dim(t37, [32, 1], [0])  # t38: \"cuda:0 f32[32, 1]\"\n",
      "        # t40 = ltorch.true_divide(t36, t38)  # t40: \"cuda:0 f32[32, 32]\"\n",
      "          # t39 = prims.broadcast_in_dim(t38, (32, 32), (0, 1))  # t39: \"cuda:0 f32[32, 32]\"\n",
      "          # t40 = prims.div(t36, t39)  # t40: \"cuda:0 f32[32, 32]\"\n",
      "      # t41 = ltorch.matmul(t40, value)  # t41: \"cuda:0 f32[32, 128]\"\n",
      "        # t41 = prims.matmul(t40, value)  # t41: \"cuda:0 f32[32, 128]\"\n",
      "  return (t41,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def test_fn(query, key, value):\n",
    "        return torch.nn.functional.scaled_dot_product_attention(query, key, value, is_causal=False)\n",
    "\n",
    "jfn = thunder.jit(test_fn)\n",
    "\n",
    "print((jfn(Q, K, V).to(\"cpu\") - test_fn(Q.to(\"cpu\"), K.to(\"cpu\"), V.to(\"cpu\"))).abs().max())\n",
    "print(thunder.last_traces(jfn)[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuda info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PyTorch CUDA Info ===\n",
      "PyTorch version: 2.6.0\n",
      "CUDA available: True\n",
      "CUDA version: 12.6\n",
      "cuDNN version: 91001\n",
      "Number of GPUs: 1\n",
      "  GPU 0: NVIDIA A10G\n",
      "    Current device: 0\n",
      "    Memory allocated: 10.97 MB\n",
      "    Memory cached   : 62.91 MB\n",
      "\n",
      "=== nvidia-smi Info (if available) ===\n",
      "Thu Aug 21 12:34:55 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.163.01             Driver Version: 550.163.01     CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A10G                    On  |   00000000:00:1E.0 Off |                    0 |\n",
      "|  0%   28C    P0             59W /  300W |    1227MiB /  23028MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "print_cuda_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
