{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flash Attention in Torch, Numba and Cuda\n",
    "\n",
    "\n",
    "We implement in three different ways the Flash Attention forward algorithm from the [Flash Attention 2 paper](https://arxiv.org/pdf/2307.08691). Namely using:\n",
    "\n",
    "1. Torch\n",
    "2. Numba\n",
    "3. Cuda\n",
    "\n",
    "- We build the kernel for `d=128` and design it so that it computes the full attention in a single block.\n",
    "\n",
    "- We then do some basic performance analysis, showing that on small matrices it performs like torch sdpa and disuss register spiling profiling.\n",
    "\n",
    "- Finally we run our custom kernel with [Cuda-python](https://developer.nvidia.com/cuda-python) and [Thunder](https://lightning.ai/docs/thunder/latest/).\n",
    "\n",
    "\n",
    "\n",
    "![./flash_attention_fwd.png](./flash_attention_fwd.png)\n",
    "\n",
    "\n",
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max absolute difference:\n",
      "O:  tensor(2.3842e-07, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import numba\n",
    "from numba.cuda import as_cuda_array as ca\n",
    "from pathlib import Path\n",
    "import math\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "from utils import load_cuda, get_sig, print_cuda_info\n",
    "\n",
    "TEST_DIMS = [\n",
    "    (32, 32),\n",
    "    (128, 64),\n",
    "    (512, 512),\n",
    "    (512, 1024),\n",
    "]\n",
    "\n",
    "def get_loaded_cuda_module(fname, verbose=False):\n",
    "    cuda_src_path = f\"./{fname}.cu\"\n",
    "    torch_src_path = f\"./torch_extension_template.cu\"\n",
    "    cuda_src = Path(cuda_src_path).read_text()\n",
    "    cuda_src += Path(torch_src_path).read_text()\n",
    "    cuda_src = cuda_src.replace(\"your_function_name\", fname)\n",
    "    cpp_src = get_sig(fname, cuda_src)\n",
    "    return load_cuda(cuda_src, cpp_src, [fname], verbose=verbose)\n",
    "\n",
    "\n",
    "def check_close(O, O_expected, L=None, L_expected=None, atol=5*1e-5):\n",
    "    O_diff = (O - O_expected).abs().max()\n",
    "    print(\"Max absolute difference:\")\n",
    "    if atol:\n",
    "        assert O_diff < atol, f\"O diff too large: {O_diff} > {atol=}\"\n",
    "    print(\"O: \", O_diff)\n",
    "    if L is not None:\n",
    "        L_diff = (L.squeeze() - L_expected).abs().max()\n",
    "        if atol:\n",
    "            assert L_diff < atol, f\"L diff too large: {L_diff} > {atol=}\"\n",
    "        print(\"L: \", L_diff)\n",
    "\n",
    "# Test tensors\n",
    "def get_test_tensors(N_inp, N_out, d):\n",
    "    Q = torch.randn(N_out, d).contiguous().to(\"cuda\")\n",
    "    K = torch.randn(N_inp, d).contiguous().to(\"cuda\")\n",
    "    V = torch.randn(N_inp, d).contiguous().to(\"cuda\")\n",
    "    scaling = 1.0 / math.sqrt(d)\n",
    "\n",
    "    # Get expected O\n",
    "    O_expected = torch.nn.functional.scaled_dot_product_attention(Q, K, V)\n",
    "    S = (Q @ K.T) * scaling  # shape: (N_out, N_inp)\n",
    "    L_expected = torch.logsumexp(S, dim=-1)\n",
    "    return Q, K, V, scaling, O_expected, L_expected\n",
    "\n",
    "# Check test tensors\n",
    "N_inp = 512\n",
    "N_out = 512\n",
    "d = 128\n",
    "\n",
    "Q, K, V, scaling, O_expected, L_expected = get_test_tensors(N_inp, N_out, d)\n",
    "check_close(O=torch.softmax(Q @ K.T * scaling, dim=-1) @ V, O_expected=O_expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flash_attention_torch(Q, K, V, O, L, N_inp, N_out, d) -> None:\n",
    "    B_c = 16\n",
    "    B_r = 16\n",
    "    T_c = (N_inp + B_c - 1) // B_c\n",
    "    T_r = (N_out + B_r - 1) // B_r\n",
    "\n",
    "    scaling = 1 / math.sqrt(d)\n",
    "\n",
    "    # Q and O L split into T_r; K, V in T_c blocks\n",
    "    for i in range(T_r):\n",
    "        Q_i = Q[i * B_r : (i + 1) * B_r]\n",
    "        O_i = torch.zeros(B_r, d)\n",
    "        L_i = torch.zeros(B_r, 1)\n",
    "        m_i = torch.full((B_r, 1), -math.inf)\n",
    "        last_m_i = m_i\n",
    "        for j in range(T_c):\n",
    "            K_j = K[j * B_c : (j + 1) * B_c]\n",
    "            V_j = V[j * B_c : (j + 1) * B_c]\n",
    "            S_i = scaling * (Q_i @ K_j.T)\n",
    "            m_i = torch.maximum(m_i, S_i.max(dim=-1, keepdim=True).values)\n",
    "            P_i = torch.exp(S_i - m_i)\n",
    "            L_i = torch.exp(last_m_i - m_i) * L_i + P_i.sum(dim=-1, keepdim=True)\n",
    "            O_i = torch.exp(last_m_i - m_i) * O_i + P_i @ V_j\n",
    "            last_m_i = m_i\n",
    "        O_i = (1.0 / L_i) * O_i\n",
    "        L_i = m_i + torch.log(L_i)\n",
    "        O[i * B_r : (i + 1) * B_r] = O_i\n",
    "        L[i * B_r : (i + 1) * B_r] = L_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max absolute difference:\n",
      "O:  tensor(4.1723e-07, device='cuda:0')\n",
      "L:  tensor(9.5367e-07, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "O_torch_loop = torch.zeros(N_out, d)\n",
    "L_torch_loop = torch.zeros(N_out, 1)\n",
    "\n",
    "flash_attention_torch(Q.to(\"cpu\"), K.to(\"cpu\"), V.to(\"cpu\"), O_torch_loop, L_torch_loop, N_inp, N_out, d)\n",
    "\n",
    "check_close(\n",
    "    O_torch_loop.to(\"cuda\"), \n",
    "    O_expected,\n",
    "    L_torch_loop.to(\"cuda\"),\n",
    "    L_expected\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numba\n",
    "\n",
    "Tiling strategy: threads cooperate loading shared arrays in each `i, j` loop and write their own entries of `l_i, m_i, O_i`\n",
    "\n",
    "### All arrays in shared memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.cuda.jit\n",
    "def flash_attention_numba_all_smem(Q, K, V, scaling: numba.float32, L, O, N_out, N_inp):\n",
    "    B_c = 16\n",
    "    B_r = 16\n",
    "    T_c = (N_inp + B_c - 1) // B_c\n",
    "    T_r = (N_out + B_r - 1) // B_r\n",
    "    inp_dtype = K.dtype\n",
    "    tid_x = numba.cuda.threadIdx.x\n",
    "    tid_y = numba.cuda.threadIdx.y\n",
    "\n",
    "    Q_i = numba.cuda.shared.array((B_r, d), inp_dtype)\n",
    "    K_j = numba.cuda.shared.array((B_c, d), inp_dtype)\n",
    "    V_j = numba.cuda.shared.array((B_c, d), inp_dtype)\n",
    "    S = numba.cuda.shared.array((B_r, B_c), inp_dtype)\n",
    "    # These can be in registers but wont fit too large\n",
    "    l_i = numba.cuda.shared.array((B_r,), inp_dtype)\n",
    "    m_i = numba.cuda.shared.array((B_r,), inp_dtype)\n",
    "    O_i = numba.cuda.shared.array((B_r, d), inp_dtype)\n",
    "\n",
    "                 \n",
    "    for i in range(T_r):\n",
    "        for ii in range(tid_y, B_r, numba.cuda.blockDim.y):\n",
    "            for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                Q_i[ii, dd] = Q[ii + i * B_r, dd]\n",
    "                O_i[ii, dd] = 0\n",
    "            l_i[ii] = 0\n",
    "            m_i[ii] = -math.inf\n",
    "        numba.cuda.syncthreads()\n",
    "\n",
    "        for j in range(T_c):\n",
    "            for jj in range(tid_y, B_c, numba.cuda.blockDim.y):\n",
    "                for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                    K_j[jj, dd] = K[jj + j * B_c, dd]\n",
    "                    V_j[jj, dd] = V[jj + j * B_c, dd]\n",
    "\n",
    "            # S[ii][jj] = scaling * (Q_i @ K_j.T)\n",
    "            numba.cuda.syncthreads()\n",
    "            for ii in range(tid_x, B_r, numba.cuda.blockDim.x):\n",
    "                for jj in range(tid_y, B_c, numba.cuda.blockDim.y):\n",
    "                    S_ij = 0\n",
    "                    for dd in range(d):\n",
    "                        S_ij += Q_i[ii, dd] * K_j[jj, dd]\n",
    "                    S_ij = scaling * S_ij\n",
    "                    S[ii][jj] = S_ij\n",
    "\n",
    "            numba.cuda.syncthreads()\n",
    "            for ii in range(tid_y, B_r, numba.cuda.blockDim.y):\n",
    "                m = m_i[ii]\n",
    "                last_m = m\n",
    "                for jj in range(B_c):\n",
    "                    m = max(m, S[ii][jj])\n",
    "                m_i[ii] = m\n",
    "                l = math.exp(last_m - m) * l_i[ii]\n",
    "\n",
    "                for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                    O_i[ii, dd] *= math.exp(last_m - m)\n",
    "                for jj in range(B_c):\n",
    "                    P_ij = math.exp(S[ii][jj] - m)\n",
    "                    l += P_ij\n",
    "                    for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                        O_i[ii, dd] += P_ij * V_j[jj, dd]\n",
    "                l_i[ii] = l\n",
    "                \n",
    "        for ii in range(tid_y, B_r, numba.cuda.blockDim.y):  \n",
    "            for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                O[ii + i * B_r, dd] = O_i[ii, dd] / l_i[ii]\n",
    "            L[ii + i * B_r] = m_i[ii] + math.log(l_i[ii])\n",
    "        \n",
    "        numba.cuda.syncthreads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max absolute difference:\n",
      "O:  tensor(2.9802e-07, device='cuda:0')\n",
      "L:  tensor(4.7684e-07, device='cuda:0')\n",
      "Max absolute difference:\n",
      "O:  tensor(3.2783e-07, device='cuda:0')\n",
      "L:  tensor(9.5367e-07, device='cuda:0')\n",
      "Max absolute difference:\n",
      "O:  tensor(6.2585e-07, device='cuda:0')\n",
      "L:  tensor(9.5367e-07, device='cuda:0')\n",
      "Max absolute difference:\n",
      "O:  tensor(5.0664e-07, device='cuda:0')\n",
      "L:  tensor(9.5367e-07, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "block_dim_x = 32\n",
    "block_dim_y = 16\n",
    "\n",
    "for N_inp, N_out in TEST_DIMS:\n",
    "\n",
    "    Q, K, V, scaling, O_expected, L_expected = get_test_tensors(N_inp, N_out, d)\n",
    "    O_all_smem = torch.zeros(N_out, d, device=\"cuda\").contiguous()\n",
    "    L_all_smem = torch.zeros(N_out, device=\"cuda\")\n",
    "    tpb = (block_dim_x, block_dim_y)\n",
    "    grid = (1,)\n",
    "    flash_attention_numba_all_smem[grid, tpb](Q, K, V, scaling, L_all_smem, O_all_smem,  N_out, N_inp)\n",
    "    torch.cuda.synchronize()\n",
    "    check_close(\n",
    "        O_all_smem, \n",
    "        O_expected,\n",
    "        L_all_smem,\n",
    "        L_expected,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving `m_i`, `l_i`, `O_i` to registers\n",
    "\n",
    "Current shared-memory usage across threads:\n",
    "```\n",
    "Shar = (B_r * d * 4) # Q_i\n",
    "+ (B_c * d * 4) # K_j\n",
    "+ (B_c * d * 4) # V_j\n",
    "+ (B_r * B_c * 4) # S\n",
    "= ~25 KB\n",
    "```\n",
    "\n",
    "Current block-shared accumulators (`m_i`, `l_i`, `O_i`):\n",
    "```\n",
    "Loc = 4 * (B_r + B_r + (B_r * d)) = 8320 B ≈ 8 KB\n",
    "```\n",
    "\n",
    "Total shared usage: **~33 KB** (fine for 1 block/SM).\n",
    "\n",
    "---\n",
    "\n",
    "**Idea:** Move `m_i`, `l_i`, `O_i` to *per-thread* locals to fit in registers.\n",
    "\n",
    "Problem: Full-size per-thread arrays would need\n",
    "\n",
    "```\n",
    "Loc * 32 * 16 ≈ 4 MB > 256 KB register memory per SM\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Optimization:** With tiling `tpb = (32, 16)`:\n",
    "\n",
    "- Each thread handles only  \n",
    "  `d // blockDim.x = 4` columns in `x`  \n",
    "  `B_r // blockDim.y = 1` row in `y`\n",
    "- So per-thread locals can be much smaller:\n",
    "\n",
    "```python\n",
    "l_i = numba.cuda.local.array((1,), inp_dtype)   # 4 B\n",
    "m_i = numba.cuda.local.array((1,), inp_dtype)   # 4 B\n",
    "O_i = numba.cuda.local.array((4,), inp_dtype)   # 16 B\n",
    "```\n",
    "-> Per-thread = 24 B, per block = 24 * 32 * 16 = 12 KB < 256 KB -> avoid register pressure and spills.\n",
    "\n",
    "\n",
    "This is how we set up `flash_attention_numba` below and the cuda version in [`./flash_attention.cu`](./flash_attention.cu)\n",
    "\n",
    "In the performance section we run `./flash_attention_spilling_from_registers.cu` that fits full arrays in local variables, to show the performance decrease by slowing the kernel by ~3×"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_dim_x = 32\n",
    "block_dim_y = 16\n",
    "B_r = 16\n",
    "B_c = 16\n",
    "d_over_dim_x = d // block_dim_x\n",
    "B_r_over_dim_y = B_r // block_dim_y\n",
    "\n",
    "@numba.cuda.jit\n",
    "def flash_attention_numba(Q, K, V, scaling: numba.float32, L, O, N_out, N_inp):\n",
    "    B_c = 16\n",
    "    B_r = 16\n",
    "    T_c = (N_inp + B_c - 1) // B_c\n",
    "    T_r = (N_out + B_r - 1) // B_r\n",
    "    inp_dtype = K.dtype\n",
    "    tid_x = numba.cuda.threadIdx.x\n",
    "    tid_y = numba.cuda.threadIdx.y\n",
    "    dim_y = numba.cuda.blockDim.y\n",
    "    dim_x = numba.cuda.blockDim.x\n",
    "    \n",
    "\n",
    "    Q_i = numba.cuda.shared.array((B_r, d), inp_dtype)\n",
    "    K_j = numba.cuda.shared.array((B_c, d), inp_dtype)\n",
    "    V_j = numba.cuda.shared.array((B_c, d), inp_dtype)\n",
    "    S = numba.cuda.shared.array((B_r, B_c), inp_dtype)\n",
    "\n",
    "    l_i = numba.cuda.local.array((B_r_over_dim_y,), inp_dtype)\n",
    "    m_i = numba.cuda.local.array((B_r_over_dim_y,), inp_dtype)\n",
    "    O_i = numba.cuda.local.array((B_r_over_dim_y, d_over_dim_x), inp_dtype)\n",
    "\n",
    "                 \n",
    "    for i in range(T_r):\n",
    "        for ii in range(tid_y, B_r, dim_y):\n",
    "            for dd in range(tid_x, d, dim_x):\n",
    "                Q_i[ii, dd] = Q[ii + i * B_r, dd]\n",
    "\n",
    "        for ii in range(B_r_over_dim_y):\n",
    "            for dd in range(d_over_dim_x):\n",
    "                O_i[ii, dd] = 0\n",
    "            l_i[ii] = 0\n",
    "            m_i[ii] = -math.inf\n",
    "        \n",
    "        numba.cuda.syncthreads()\n",
    "        for j in range(T_c):\n",
    "            for jj in range(tid_y, B_c, dim_y):\n",
    "                for dd in range(tid_x, d, dim_x):\n",
    "                    K_j[jj, dd] = K[jj + j * B_c, dd]\n",
    "                    V_j[jj, dd] = V[jj + j * B_c, dd]\n",
    "\n",
    "            # S[ii][jj] = scaling * (Q_i @ K_j.T)\n",
    "            numba.cuda.syncthreads()\n",
    "            for ii in range(tid_x, B_r, dim_x):\n",
    "                for jj in range(tid_y, B_c, dim_y):\n",
    "                    S_ij = 0\n",
    "                    for dd in range(d):\n",
    "                        S_ij += Q_i[ii, dd] * K_j[jj, dd]\n",
    "                    S_ij = scaling * S_ij\n",
    "                    S[ii][jj] = S_ij\n",
    "\n",
    "            numba.cuda.syncthreads()\n",
    "            for ii in range(B_r_over_dim_y):\n",
    "                m = m_i[ii]\n",
    "                last_m = m\n",
    "                for jj in range(B_c):\n",
    "                    m = max(m, S[ii * dim_y + tid_y][jj])\n",
    "                m_i[ii] = m\n",
    "                l = numba.float32(math.exp(last_m - m)) * l_i[ii]\n",
    "\n",
    "                for dd in range(d_over_dim_x):\n",
    "                    O_i[ii, dd] *= numba.float32(math.exp(last_m - m))\n",
    "                for jj in range(B_c):\n",
    "                    P_ij = numba.float32(math.exp(S[ii * dim_y + tid_y][jj] - m))\n",
    "                    l += P_ij\n",
    "                    for dd in range(d_over_dim_x):\n",
    "                        O_i[ii, dd] += P_ij * V_j[jj, dd * dim_x + tid_x]\n",
    "                l_i[ii] = l\n",
    "                \n",
    "        for ii in range(B_r_over_dim_y):  \n",
    "            for dd in range(d_over_dim_x):\n",
    "                O[ii * dim_y + tid_y + i * B_r, dd * dim_x + tid_x] = O_i[ii, dd] / l_i[ii]\n",
    "            L[ii * dim_y + tid_y + i * B_r] = m_i[ii] + numba.float32(math.log(l_i[ii]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max absolute difference:\n",
      "O:  tensor(4.1723e-07, device='cuda:0')\n",
      "L:  tensor(4.7684e-07, device='cuda:0')\n",
      "Max absolute difference:\n",
      "O:  tensor(6.5565e-07, device='cuda:0')\n",
      "L:  tensor(9.5367e-07, device='cuda:0')\n",
      "Max absolute difference:\n",
      "O:  tensor(4.9174e-07, device='cuda:0')\n",
      "L:  tensor(1.4305e-06, device='cuda:0')\n",
      "Max absolute difference:\n",
      "O:  tensor(4.7684e-07, device='cuda:0')\n",
      "L:  tensor(1.4305e-06, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for N_inp, N_out in TEST_DIMS:\n",
    "    Q, K, V, scaling, O_expected, L_expected = get_test_tensors(N_inp, N_out, d)\n",
    "\n",
    "    O_all_smem = torch.zeros(N_out, d, device=\"cuda\").contiguous()\n",
    "    L_all_smem = torch.zeros(N_out, device=\"cuda\")\n",
    "    tpb = (block_dim_x, block_dim_y)\n",
    "    grid = (1,)\n",
    "    flash_attention_numba[grid, tpb](Q, K, V, scaling, L_all_smem, O_all_smem,  N_out, N_inp)\n",
    "    check_close(\n",
    "        O_all_smem, \n",
    "        O_expected,\n",
    "        L_all_smem,\n",
    "        L_expected,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuda\n",
    "\n",
    "### flash_attention_numba in Cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2073: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "fname = \"flash_attention\"\n",
    "module = get_loaded_cuda_module(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max absolute difference:\n",
      "O:  tensor(4.7684e-07, device='cuda:0')\n",
      "L:  tensor(4.7684e-07, device='cuda:0')\n",
      "Max absolute difference:\n",
      "O:  tensor(4.1723e-07, device='cuda:0')\n",
      "L:  tensor(4.7684e-07, device='cuda:0')\n",
      "Max absolute difference:\n",
      "O:  tensor(5.0664e-07, device='cuda:0')\n",
      "L:  tensor(9.5367e-07, device='cuda:0')\n",
      "Max absolute difference:\n",
      "O:  tensor(5.0664e-07, device='cuda:0')\n",
      "L:  tensor(1.4305e-06, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for N_inp, N_out in TEST_DIMS:\n",
    "    Q, K, V, _, O_expected, L_expected = get_test_tensors(N_inp, N_out, d)\n",
    "    O_move_registers, L_move_registers = getattr(module, fname)(Q, K, V)\n",
    "    check_close(\n",
    "        O_move_registers, \n",
    "        O_expected,\n",
    "        L_move_registers,\n",
    "        L_expected,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance \n",
    "\n",
    "Run timeit on different dimensions. \n",
    "\n",
    "- Recall that we build the kernel for `d=128` and design it so that it computes the full attention in a single block.\n",
    "\n",
    "- For matrices with small `N_out` this implementation is comparable with `scaled_dot_product_attention` faster backends.\n",
    "\n",
    "- But by making N larger this implementation slows down dramatically it only uses a single block.\n",
    "\n",
    "- Note that the register splilling version is always much slower than `scaled_dot_product_attention`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load registers spilling version\n",
    "\n",
    "This is the version loading full arrays as local variables in threads, which leads to spilling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2073: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max absolute difference:\n",
      "O:  tensor(5.0664e-07, device='cuda:0')\n",
      "L:  tensor(1.4305e-06, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "fname_spill_from_registers = \"flash_attention_spilling_from_registers\"\n",
    "module_spilling_from_registers = get_loaded_cuda_module(fname_spill_from_registers)\n",
    "O_cuda_spilling, L_cuda_spilling = getattr(module_spilling_from_registers, fname_spill_from_registers)(Q, K, V)\n",
    "check_close(O_cuda_spilling, O_expected, L_cuda_spilling, L_expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timeit: sdpa backends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Explicit softmax(Q @ K.T * scaling) @ V\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.5 μs ± 583 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "\n",
      "Backend: None\n",
      "83.8 μs ± 287 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "\n",
      "Backend: EFFICIENT_ATTENTION\n",
      "67.8 μs ± 263 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "\n",
      "Backend: FLASH_ATTENTION (f16)\n",
      "70.3 μs ± 450 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "\n",
      "Backend: CUDNN_ATTENTION (f16)\n",
      "94 μs ± 30.5 μs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# Benchmark first the different backends for scaled_dot_product_attention\n",
    "# For small tensors\n",
    "from torch.nn.attention import sdpa_kernel, SDPBackend\n",
    "\n",
    "Q, K, V, _, _, _ = get_test_tensors(N_inp=32, N_out=32, d=128)\n",
    "Qhalf, Khalf, Vhalf = Q.to(torch.float16), K.to(torch.float16), V.to(torch.float16)\n",
    "\n",
    "def run_sdpa_with_backend(Q, K, V, backend):\n",
    "    with sdpa_kernel(backends=backend):\n",
    "        torch.nn.functional.scaled_dot_product_attention(\n",
    "            Q.unsqueeze(0).unsqueeze(0), \n",
    "            K.unsqueeze(0).unsqueeze(0), \n",
    "            V.unsqueeze(0).unsqueeze(0),\n",
    "        )\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "print(\"\\nExplicit softmax(Q @ K.T * scaling) @ V\")\n",
    "%timeit torch.softmax(Q @ K.T * scaling, dim=-1) @ V; torch.cuda.synchronize()\n",
    "print(\"\\nBackend: None\")\n",
    "%timeit torch.nn.functional.scaled_dot_product_attention(Q, K, V)\n",
    "print(\"\\nBackend: EFFICIENT_ATTENTION\")\n",
    "%timeit run_sdpa_with_backend(Q, K, V, backend=SDPBackend.EFFICIENT_ATTENTION)\n",
    "print(\"\\nBackend: FLASH_ATTENTION (f16)\")\n",
    "%timeit run_sdpa_with_backend(Qhalf, Khalf, Vhalf, backend=SDPBackend.FLASH_ATTENTION)\n",
    "print(\"\\nBackend: CUDNN_ATTENTION (f16)\")\n",
    "%timeit run_sdpa_with_backend(Qhalf, Khalf, Vhalf, backend=SDPBackend.CUDNN_ATTENTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timeit: sdpa vs custom kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "**********\n",
      "Dimensions: N_out=32, N_inp=32, d=128\n",
      "\n",
      "- Torch scaled_dot_product_attention\n",
      "105 μs ± 545 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "\n",
      "- Custom Flash Attention\n",
      "64.1 μs ± 325 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "\n",
      "- Custom Flash Attention: spill from registers\n",
      "314 μs ± 824 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "\n",
      "\n",
      "**********\n",
      "Dimensions: N_out=64, N_inp=128, d=128\n",
      "\n",
      "- Torch scaled_dot_product_attention\n",
      "106 μs ± 820 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "\n",
      "- Custom Flash Attention\n",
      "298 μs ± 520 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "\n",
      "- Custom Flash Attention: spill from registers\n",
      "2.26 ms ± 1.95 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "\n",
      "**********\n",
      "Dimensions: N_out=512, N_inp=512, d=128\n",
      "\n",
      "- Torch scaled_dot_product_attention\n",
      "114 μs ± 672 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "\n",
      "- Custom Flash Attention\n",
      "8.46 ms ± 1.27 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "- Custom Flash Attention: spill from registers\n",
      "70.8 ms ± 9.11 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "\n",
      "**********\n",
      "Dimensions: N_out=1024, N_inp=512, d=128\n",
      "\n",
      "- Torch scaled_dot_product_attention\n",
      "110 μs ± 313 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "\n",
      "- Custom Flash Attention\n",
      "16.9 ms ± 1.48 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "- Custom Flash Attention: spill from registers\n",
      "142 ms ± 7.26 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Check now run against \n",
    "for N_inp, N_out in TEST_DIMS:\n",
    "    Q, K, V, _, _, _ = get_test_tensors(N_inp, N_out, d)\n",
    "    print(f\"\\n\\n**********\\nDimensions: {N_out=}, {N_inp=}, {d=}\")\n",
    "    torch.cuda.synchronize()\n",
    "    print(\"\\n- Torch scaled_dot_product_attention\")\n",
    "    %timeit torch.nn.functional.scaled_dot_product_attention(Q, K, V); torch.cuda.synchronize()\n",
    "    print(\"\\n- Custom Flash Attention\")\n",
    "    %timeit getattr(module, fname)(Q, K, V); torch.cuda.synchronize()\n",
    "    print(\"\\n- Custom Flash Attention: spill from registers\")\n",
    "    %timeit getattr(module_spilling_from_registers, fname_spill_from_registers)(Q, K, V); torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profile\n",
    "\n",
    "We can compare the performance of the kernel with and without register spilling as follows:\n",
    "```\n",
    "# Get ptx files or use https://godbolt.org/\n",
    "nvcc -ptx flash_attention.cu -o flash_attention.ptx\n",
    "nvcc -ptx flash_attention_spilling_from_registers.cu -o flash_attention_spilling_from_registers.ptx\n",
    "\n",
    "# Get ncu metrics\n",
    "nvcc -O3 -o test_attention main.cu flash_attention.cu flash_attention_spilling_from_registers.cu\n",
    "ncu ./test_attention\n",
    "```\n",
    "\n",
    "#### PTX comparison\n",
    "\n",
    "For the [spilling kernel](./flash_attention_spilling_from_registers.cu) we see \n",
    "```ptx\n",
    ".local .align 16 .b8 \t__local_depot0[8320];\n",
    "```\n",
    "where 8320 B is exactly the allocation for\n",
    "```cu\n",
    "float l_i[B_r];\n",
    "float m_i[B_r];\n",
    "float O_i[B_r][d];\n",
    "```\n",
    "\n",
    "\n",
    "#### Nsight Compute Comparison\n",
    "\n",
    "| **Metric** | **Spilling Kernel (`flash_attention_spilling_from_registers_k`)** | **Non-Spilling Kernel (`flash_attention_k`)** | **Difference** |\n",
    "|------------|------------------------------------------------------------------|-----------------------------------------------|---------------------------------|\n",
    "| **Duration** | **13.02 ms** | **2.10 ms** | Spilling kernel is ~6× slower |\n",
    "| **Compute (SM) Throughput** | **0.26%** | **1.20%** | 5× higher compute utilization in non-spilling kernel |\n",
    "| **L2 Cache Throughput** | **1.71%** | **0.56%** | Spilling kernel hits L2 more |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuda-Python\n",
    "\n",
    "`conda install conda-forge::cuda-python`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "\u0000\n",
      "0\n",
      "0 <CUfunction 0x55c22a3d7df0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap_external>:1301: FutureWarning: The cuda.cuda module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.driver module instead.\n",
      "<frozen importlib._bootstrap_external>:1301: FutureWarning: The cuda.nvrtc module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.nvrtc module instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([94292422803264, 94292422803272, 94292422803280, 94292422803288,\n",
       "        94292422803296, 94292425259776, 94292425564544, 94292425564548,\n",
       "        94292425564552], dtype=torch.uint64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create program\n",
    "from cuda import cuda, nvrtc\n",
    "\n",
    "cuda_src_path = f\"./flash_attention.cu\"\n",
    "cuda_src = Path(cuda_src_path).read_text()\n",
    "\n",
    "N_inp = 32\n",
    "N_out = 32\n",
    "d = 128\n",
    "B_r, B_c = 16, 16\n",
    "T_r = (N_out + B_r -1) // B_r\n",
    "T_c = (N_inp + B_r -1) // B_c\n",
    "Q, K, V, scale_factor, O_expected, L_expected = get_test_tensors(N_inp, N_out, d)\n",
    "\n",
    "err, prog = nvrtc.nvrtcCreateProgram(str.encode(cuda_src), b\"flash_attention.cu\", 0, [], [])\n",
    "\n",
    "# Compile program\n",
    "min, maj = torch.cuda.get_device_capability()\n",
    "opts = [\n",
    "    f\"--gpu-architecture=compute_{min}{maj}\".encode(), \n",
    "    \"--device-as-default-execution-space\".encode(),\n",
    "    \"--std=c++14\".encode()]\n",
    "err, = nvrtc.nvrtcCompileProgram(prog, len(opts), opts)\n",
    "\n",
    "print(err)\n",
    "\n",
    "# Get PTX from compilation\n",
    "err, ptxSize = nvrtc.nvrtcGetPTXSize(prog)\n",
    "ptx = b\" \" * ptxSize\n",
    "err, = nvrtc.nvrtcGetPTX(prog, ptx)\n",
    "print(err)\n",
    "\n",
    "err, logSize = nvrtc.nvrtcGetProgramLogSize(prog)\n",
    "log = b\" \" * logSize\n",
    "err, = nvrtc.nvrtcGetProgramLog(prog, log)\n",
    "print(log.decode())\n",
    "# print(ptx.decode())\n",
    "\n",
    "# Load PTX as module data and retrieve function\n",
    "err, module = cuda.cuModuleLoadData(ptx)\n",
    "print(err)\n",
    "err, kernel = cuda.cuModuleGetFunction(module, b\"flash_attention_k\")\n",
    "print(err, kernel)\n",
    "\n",
    "# Allocate tensors\n",
    "# S3 = torch.zeros(N_out, N_out, device=\"cuda\")\n",
    "O_cuda_py = torch.zeros(N_out, d, device=\"cuda\")\n",
    "L_cuda_py = torch.zeros(N_out, device=\"cuda\")\n",
    "\n",
    "# To quote the official tutorial: (https://nvidia.github.io/cuda-python/overview.html)\n",
    "# The following code example is not intuitive\n",
    "# Subject to change in a future release\n",
    "\n",
    "int_args = torch.tensor([0, T_r, T_c], dtype=torch.int32)\n",
    "float_args = torch.tensor([scale_factor], dtype=torch.float32)\n",
    "ptr_args = torch.tensor([i.data_ptr() for i in (O_cuda_py, L_cuda_py, Q, K, V)], dtype=torch.uint64)\n",
    "\n",
    "args = torch.tensor([\n",
    "    *(i.data_ptr() for i in ptr_args),\n",
    "    *(i.data_ptr() for i in float_args),\n",
    "    *(i.data_ptr() for i in int_args)], dtype=torch.uint64)\n",
    "\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.3447e-07, device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fn():\n",
    "    err = cuda.cuLaunchKernel(\n",
    "        kernel,\n",
    "        1,  # grid x dim\n",
    "        1,  # grid y dim\n",
    "        1,  # grid z dim\n",
    "        16,  # block x dim\n",
    "        16,  # block y dim\n",
    "        1,  # block z dim\n",
    "        0,  # dynamic shared memory\n",
    "        torch.cuda.current_stream().stream_id,  # stream\n",
    "        args.data_ptr(),  # kernel arguments\n",
    "        0,  # extra (ignore)\n",
    "    )\n",
    "\n",
    "fn()\n",
    "\n",
    "(O_cuda_py - O_expected).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "**********\n",
      "Dimensions: N_out=32, N_inp=32, d=128\n",
      "\n",
      "- Custom Flash Attention: Cuda-python\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.4 μs ± 251 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n\\n**********\\nDimensions: {N_out=}, {N_inp=}, {d=}\")\n",
    "torch.cuda.synchronize()\n",
    "print(\"\\n- Custom Flash Attention: Cuda-python\")\n",
    "%timeit fn(); torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thunder\n",
    "\n",
    "[Installation guide](https://lightning.ai/docs/thunder/latest/fundamentals/installation.html)\n",
    "\n",
    "We use thunder here to include our custom kernel as an option within torch's own `scaled_dot_product_attention`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/thunder/executors/transformer_engine_v2ex.py:25: UserWarning: transformer_engine module not found!\n",
      "  warnings.warn(\"transformer_engine module not found!\")\n"
     ]
    }
   ],
   "source": [
    "import thunder\n",
    "\n",
    "attn_ex = thunder.extend.OperatorExecutor('attn_ex', version=0.01)\n",
    "thunder.add_default_executor(attn_ex)\n",
    "\n",
    "# [attn_ex, attn_ex, sdpa, nvfuser]\n",
    "\n",
    "def my_attn_impl(query, key, value, scale):\n",
    "    n_out, d = query.shape\n",
    "\n",
    "    # S3 = torch.zeros(N_out, N_out, device=\"cuda\")\n",
    "    O3 = torch.zeros(N_out, d, device=\"cuda\")\n",
    "    L3 = torch.zeros(N_out, device=\"cuda\")\n",
    "\n",
    "    B_c = 16\n",
    "    B_r = 16\n",
    "    T_c = (N_inp + B_c - 1) // B_c\n",
    "    T_r = (N_out + B_r - 1) // B_r\n",
    "\n",
    "    int_args = torch.tensor([N_out, T_r, T_c], dtype=torch.int32)\n",
    "    float_args = torch.tensor([scale_factor], dtype=torch.float32)\n",
    "    ptr_args = torch.tensor([i.data_ptr() for i in (O3, L3, key, query, value)], dtype=torch.uint64)\n",
    "\n",
    "    args = torch.tensor([\n",
    "        *(i.data_ptr() for i in ptr_args),\n",
    "        *(i.data_ptr() for i in float_args),\n",
    "        *(i.data_ptr() for i in int_args)], dtype=torch.uint64\n",
    "    )\n",
    "\n",
    "    err, _ = cuda.cuLaunchKernel(\n",
    "        kernel,\n",
    "        1,  # grid x dim\n",
    "        1,  # grid y dim\n",
    "        1,  # grid z dim\n",
    "        32, # block x dim\n",
    "        32, # block y dim\n",
    "        1,  # block z dim\n",
    "        0,  # dynamic shared memory\n",
    "        torch.cuda.current_stream().stream_id,  # stream\n",
    "        args.data_ptr(),  # kernel arguments\n",
    "        0,  # extra (ignore)\n",
    "    )\n",
    "    assert err == cuda.CUresult.CUDA_SUCCESS, err\n",
    "    return O3, L3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Register our implementation as an operator\n",
    "def my_attn_meta(query, key, value, scale):\n",
    "    return thunder.TensorProxy(like=query), thunder.TensorProxy(like=query, shape=(query.shape[:-1],))\n",
    "\n",
    "my_attn = attn_ex.register_operator('my_attn', meta=my_attn_meta, fn=my_attn_impl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_attn_checker(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False, scale=None):\n",
    "    if attn_mask is not None or dropout_p == 0.0 or is_causal:\n",
    "        return False\n",
    "    if len(query.shape) > 2:\n",
    "            return (query.device.device_type == thunder.devices.DeviceType.CUDA and\n",
    "                key.device == query.device and\n",
    "                value.device == query.device)\n",
    "    return False\n",
    "\n",
    "def my_attn_transform(query, key, value, attn_masks=None, dropout_p=0.0, is_causal=False, scale=None):\n",
    "    if scale is None:\n",
    "        scale = query.size(-1) ** -0.5\n",
    "    out = my_attn(query, key, value, scale)\n",
    "    return out[0]\n",
    "\n",
    "attn_ex.register_implementation(thunder.torch.scaled_dot_product_attention, checker=my_attn_checker,\n",
    "                                  execution_transform=my_attn_transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., device='cuda:0')\n",
      "# Constructed by Unwrap the actual return value\n",
      "import torch\n",
      "import torch.nn.functional\n",
      "from thunder.executors.torchex import no_autocast\n",
      "\n",
      "@torch.no_grad()\n",
      "@no_autocast\n",
      "def computation(query, key, value):\n",
      "  # query: \"cuda:0 f32[32, 128]\"\n",
      "  # key: \"cuda:0 f32[32, 128]\"\n",
      "  # value: \"cuda:0 f32[32, 128]\"\n",
      "\n",
      "  # /tmp/ipykernel_18400/3826419770.py:2: \t        return torch.nn.functional.scaled_dot_product_attention(query, key, value, is_causal=False)\n",
      "  t41 = torch.nn.functional.scaled_dot_product_attention(query, key, value, None, 0.0, False, scale=None)  # t41: \"cuda:0 f32[32, 128]\"\n",
      "    # t41 = ltorch.scaled_dot_product_attention(query, key, value, None, 0.0, False, scale=None)  # t41: \"cuda:0 f32[32, 128]\"\n",
      "      # t28 = ltorch.mul(query, 0.29730177875068026)  # t28: \"cuda:0 f32[32, 128]\"\n",
      "        # t28 = prims.mul(query, 0.29730177875068026)  # t28: \"cuda:0 f32[32, 128]\"\n",
      "      # t29 = ltorch.transpose(key, -2, -1)  # t29: \"cuda:0 f32[128, 32]\"\n",
      "        # t29 = prims.transpose(key, (1, 0))  # t29: \"cuda:0 f32[128, 32]\"\n",
      "      # t30 = ltorch.mul(t29, 0.29730177875068026)  # t30: \"cuda:0 f32[128, 32]\"\n",
      "        # t30 = prims.mul(t29, 0.29730177875068026)  # t30: \"cuda:0 f32[128, 32]\"\n",
      "      # t31 = ltorch.matmul(t28, t30)  # t31: \"cuda:0 f32[32, 32]\"\n",
      "        # t31 = prims.matmul(t28, t30)  # t31: \"cuda:0 f32[32, 32]\"\n",
      "      # t40 = ltorch._softmax(t31, -1, dtype=None)  # t40: \"cuda:0 f32[32, 32]\"\n",
      "        # t33 = ltorch.amax(t31, -1, True)  # t33: \"cuda:0 f32[32, 1]\"\n",
      "          # t32 = prims.amax(t31, (1,))  # t32: \"cuda:0 f32[32]\"\n",
      "          # t33 = prims.broadcast_in_dim(t32, [32, 1], [0])  # t33: \"cuda:0 f32[32, 1]\"\n",
      "        # t35 = ltorch.sub(t31, t33, alpha=1)  # t35: \"cuda:0 f32[32, 32]\"\n",
      "          # t34 = prims.broadcast_in_dim(t33, (32, 32), (0, 1))  # t34: \"cuda:0 f32[32, 32]\"\n",
      "          # t35 = prims.sub(t31, t34)  # t35: \"cuda:0 f32[32, 32]\"\n",
      "        # t36 = ltorch.exp(t35)  # t36: \"cuda:0 f32[32, 32]\"\n",
      "          # t36 = prims.exp(t35)  # t36: \"cuda:0 f32[32, 32]\"\n",
      "        # t38 = ltorch.sum(t36, -1, True, dtype=None)  # t38: \"cuda:0 f32[32, 1]\"\n",
      "          # t37 = prims.sum(t36, (1,))  # t37: \"cuda:0 f32[32]\"\n",
      "          # t38 = prims.broadcast_in_dim(t37, [32, 1], [0])  # t38: \"cuda:0 f32[32, 1]\"\n",
      "        # t40 = ltorch.true_divide(t36, t38)  # t40: \"cuda:0 f32[32, 32]\"\n",
      "          # t39 = prims.broadcast_in_dim(t38, (32, 32), (0, 1))  # t39: \"cuda:0 f32[32, 32]\"\n",
      "          # t40 = prims.div(t36, t39)  # t40: \"cuda:0 f32[32, 32]\"\n",
      "      # t41 = ltorch.matmul(t40, value)  # t41: \"cuda:0 f32[32, 128]\"\n",
      "        # t41 = prims.matmul(t40, value)  # t41: \"cuda:0 f32[32, 128]\"\n",
      "  return (t41,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def test_fn(query, key, value):\n",
    "        return torch.nn.functional.scaled_dot_product_attention(query, key, value, is_causal=False)\n",
    "\n",
    "jfn = thunder.jit(test_fn)\n",
    "\n",
    "print((jfn(Q, K, V) - test_fn(Q, K, V)).abs().max())\n",
    "print(thunder.last_traces(jfn)[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuda info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PyTorch CUDA Info ===\n",
      "PyTorch version: 2.6.0\n",
      "CUDA available: True\n",
      "CUDA version: 12.6\n",
      "cuDNN version: 91001\n",
      "Number of GPUs: 1\n",
      "  GPU 0: NVIDIA A10G\n",
      "    Current device: 0\n",
      "    Memory allocated: 10.22 MB\n",
      "    Memory cached   : 29.36 MB\n",
      "\n",
      "=== nvidia-smi Info (if available) ===\n",
      "Wed Aug 20 13:46:17 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.163.01             Driver Version: 550.163.01     CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A10G                    On  |   00000000:00:1E.0 Off |                    0 |\n",
      "|  0%   40C    P0             66W /  300W |    1195MiB /  23028MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "print_cuda_info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
