{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flash Attention in Torch, Numba and Cuda\n",
    "\n",
    "\n",
    "We implement in 3 different ways the forward algorithm from the [Flash Attention 2 paper](https://arxiv.org/pdf/2307.08691):\n",
    "\n",
    "1. Torch operations\n",
    "2. Numba\n",
    "3. Cuda\n",
    "\n",
    "We do some basic performance analysis as well as running the custom kernel with cuda-python and thunder.\n",
    "\n",
    "\n",
    "- We build the kernel for `d=128` and design it so that it computes the full attention in a single block.\n",
    "\n",
    "![./flash_attention_fwd.png](./flash_attention_fwd.png)\n",
    "\n",
    "\n",
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "from numba.cuda import as_cuda_array as ca\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import sys, os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "from utils import load_cuda, get_sig, print_cuda_info\n",
    "\n",
    "import os\n",
    "\n",
    "TEST_DIMS = [\n",
    "    (32, 32),\n",
    "    (128, 64),\n",
    "    (512, 512),\n",
    "    (512, 1024),\n",
    "]\n",
    "\n",
    "def get_loaded_cuda_module(fname, verbose=False):\n",
    "    cuda_src_path = f\"./{fname}.cu\"\n",
    "    torch_src_path = f\"./torch_extension_template.cu\"\n",
    "    cuda_src = Path(cuda_src_path).read_text()\n",
    "    cuda_src += Path(torch_src_path).read_text()\n",
    "    cuda_src = cuda_src.replace(\"your_function_name\", fname)\n",
    "    cpp_src = get_sig(fname, cuda_src)\n",
    "    return load_cuda(cuda_src, cpp_src, [fname], verbose=verbose)\n",
    "\n",
    "\n",
    "def check_close(O, O_expected, L=None, L_expected=None, atol=5*1e-5):\n",
    "    O_diff = (O-O_expected).abs().max()\n",
    "    print(\"Max absolute difference:\")\n",
    "    if atol:\n",
    "        assert O_diff < atol, f\"O diff too large: {O_diff} > {atol=}\"\n",
    "    print(\"O: \", O_diff)\n",
    "    if L is not None:\n",
    "        L_diff = (L.squeeze()-L_expected).abs().max()\n",
    "        if atol:\n",
    "            assert L_diff < atol, f\"L diff too large: {L_diff} > {atol=}\"\n",
    "        print(\"L: \", L_diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max absolute difference:\n",
      "O:  tensor(5.6624e-07, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Test tensors\n",
    "def get_test_tensors(N_inp, N_out, d):\n",
    "    Q = torch.randn(N_out, d).contiguous().to(\"cuda\")\n",
    "    K = torch.randn(N_inp, d).contiguous().to(\"cuda\")\n",
    "    V = torch.randn(N_inp, d).contiguous().to(\"cuda\")\n",
    "    scaling = 1.0 / math.sqrt(d)\n",
    "\n",
    "    # Get expected O\n",
    "    O_expected = torch.nn.functional.scaled_dot_product_attention(Q, K, V)\n",
    "    S = (Q @ K.T) * scaling  # shape: (N_out, N_inp)\n",
    "    L_expected = torch.logsumexp(S, dim=-1)\n",
    "    return Q, K, V, scaling, O_expected, L_expected\n",
    "\n",
    "N_inp = 512\n",
    "N_out = 512\n",
    "d = 128\n",
    "\n",
    "Q, K, V, scaling, O_expected, L_expected = get_test_tensors(N_inp, N_out, d)\n",
    "check_close(O=torch.softmax(Q @ K.T * scaling, dim=-1) @ V, O_expected=O_expected)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flash_attention_torch(Q, K, V, O, L, N_inp, N_out, d) -> None:\n",
    "    \"\"\"Forward algo from https://arxiv.org/pdf/2307.08691\n",
    "    \"\"\"\n",
    "\n",
    "    B_c = 16\n",
    "    B_r = 16\n",
    "    T_c = (N_inp + B_c - 1) // B_c\n",
    "    T_r = (N_out + B_r - 1) // B_r\n",
    "\n",
    "    scaling = 1 / math.sqrt(d)\n",
    "\n",
    "    # Q and O L split into T_r; K, V in T_c blocks\n",
    "    for i in range(T_r):\n",
    "        Q_i = Q[i * B_r : (i + 1) * B_r]\n",
    "        O_i = torch.zeros(B_r, d)\n",
    "        L_i = torch.zeros(B_r, 1)\n",
    "        m_i = torch.full((B_r, 1), -math.inf)\n",
    "        last_m_i = m_i\n",
    "        for j in range(T_c):\n",
    "            K_j = K[j * B_c : (j + 1) * B_c]\n",
    "            V_j = V[j * B_c : (j + 1) * B_c]\n",
    "            S_i = scaling * (Q_i @ K_j.T)\n",
    "            m_i = torch.maximum(m_i, S_i.max(dim=-1, keepdim=True).values)\n",
    "            P_i = torch.exp(S_i - m_i)\n",
    "            L_i = torch.exp(last_m_i - m_i) * L_i + P_i.sum(dim=-1, keepdim=True)\n",
    "            O_i = torch.exp(last_m_i - m_i) * O_i + P_i @ V_j\n",
    "            last_m_i = m_i\n",
    "        O_i = (1.0 / L_i) * O_i\n",
    "        L_i = m_i + torch.log(L_i)\n",
    "        O[i * B_r : (i + 1) * B_r] = O_i\n",
    "        L[i * B_r : (i + 1) * B_r] = L_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max absolute difference:\n",
      "O:  tensor(5.9605e-07, device='cuda:0')\n",
      "L:  tensor(9.5367e-07, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "O_torch_loop = torch.zeros(N_out, d)\n",
    "L_torch_loop = torch.zeros(N_out, 1)\n",
    "\n",
    "flash_attention_torch(Q.to(\"cpu\"), K.to(\"cpu\"), V.to(\"cpu\"), O_torch_loop, L_torch_loop, N_inp, N_out, d)\n",
    "\n",
    "check_close(\n",
    "    O_torch_loop.to(\"cuda\"), \n",
    "    O_expected,\n",
    "    L_torch_loop.to(\"cuda\"),\n",
    "    L_expected\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numba\n",
    "\n",
    "Tiling strategy: each thread computes one value in\n",
    "\n",
    "### All arrays in shared memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.cuda.jit\n",
    "def flash_attention_numba_all_smem(Q, K, V, scaling: numba.float32, L, O, N_out, N_inp):\n",
    "    B_c = 16\n",
    "    B_r = 16\n",
    "    T_c = (N_inp + B_c - 1) // B_c\n",
    "    T_r = (N_out + B_r - 1) // B_r\n",
    "    inp_dtype = K.dtype\n",
    "    tid_x = numba.cuda.threadIdx.x\n",
    "    tid_y = numba.cuda.threadIdx.y\n",
    "\n",
    "    Q_i = numba.cuda.shared.array((B_r, d), inp_dtype)\n",
    "    K_j = numba.cuda.shared.array((B_c, d), inp_dtype)\n",
    "    V_j = numba.cuda.shared.array((B_c, d), inp_dtype)\n",
    "    S = numba.cuda.shared.array((B_r, B_c), inp_dtype)\n",
    "    # These can be in registers but wont fit too large\n",
    "    l_i = numba.cuda.shared.array((B_r,), inp_dtype)\n",
    "    m_i = numba.cuda.shared.array((B_r,), inp_dtype)\n",
    "    O_i = numba.cuda.shared.array((B_r, d), inp_dtype)\n",
    "\n",
    "                 \n",
    "    for i in range(T_r):\n",
    "        for ii in range(tid_y, B_r, numba.cuda.blockDim.y):\n",
    "            for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                Q_i[ii, dd] = Q[ii + i * B_r, dd]\n",
    "                O_i[ii, dd] = 0\n",
    "            l_i[ii] = 0\n",
    "            m_i[ii] = -math.inf\n",
    "        numba.cuda.syncthreads()\n",
    "\n",
    "        for j in range(T_c):\n",
    "            for jj in range(tid_y, B_c, numba.cuda.blockDim.y):\n",
    "                for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                    K_j[jj, dd] = K[jj + j * B_c, dd]\n",
    "                    V_j[jj, dd] = V[jj + j * B_c, dd]\n",
    "\n",
    "            # S[ii][jj] = scaling * (Q_i @ K_j.T)\n",
    "            numba.cuda.syncthreads()\n",
    "            for ii in range(tid_x, B_r, numba.cuda.blockDim.x):\n",
    "                for jj in range(tid_y, B_c, numba.cuda.blockDim.y):\n",
    "                    S_ij = 0\n",
    "                    for dd in range(d):\n",
    "                        S_ij += Q_i[ii, dd] * K_j[jj, dd]\n",
    "                    S_ij = scaling * S_ij\n",
    "                    S[ii][jj] = S_ij\n",
    "\n",
    "            numba.cuda.syncthreads()\n",
    "            for ii in range(tid_y, B_r, numba.cuda.blockDim.y):\n",
    "                m = m_i[ii]\n",
    "                last_m = m\n",
    "                for jj in range(B_c):\n",
    "                    m = max(m, S[ii][jj])\n",
    "                m_i[ii] = m\n",
    "                l = math.exp(last_m - m) * l_i[ii]\n",
    "\n",
    "                for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                    O_i[ii, dd] *= math.exp(last_m - m)\n",
    "                for jj in range(B_c):\n",
    "                    P_ij = math.exp(S[ii][jj] - m)  # Cache...\n",
    "                    l += P_ij\n",
    "                    for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                        O_i[ii, dd] += P_ij * V_j[jj, dd]\n",
    "                l_i[ii] = l\n",
    "                \n",
    "        numba.cuda.syncthreads()\n",
    "        for ii in range(tid_y, B_r, numba.cuda.blockDim.y):  \n",
    "            for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                O[ii + i * B_r, dd] = O_i[ii, dd] / l_i[ii]\n",
    "            L[ii + i * B_r] = m_i[ii] + math.log(l_i[ii])\n",
    "        numba.cuda.syncthreads() \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max absolute difference:\n",
      "O:  tensor(4.7684e-07, device='cuda:0')\n",
      "L:  tensor(4.7684e-07, device='cuda:0')\n",
      "Max absolute difference:\n",
      "O:  tensor(3.8743e-07, device='cuda:0')\n",
      "L:  tensor(9.5367e-07, device='cuda:0')\n",
      "Max absolute difference:\n",
      "O:  tensor(7.7486e-07, device='cuda:0')\n",
      "L:  tensor(1.4305e-06, device='cuda:0')\n",
      "Max absolute difference:\n",
      "O:  tensor(6.5565e-07, device='cuda:0')\n",
      "L:  tensor(1.4305e-06, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "block_dim_x = 32\n",
    "block_dim_y = 16\n",
    "\n",
    "for N_inp, N_out in TEST_DIMS:\n",
    "\n",
    "    Q, K, V, scaling, O_expected, L_expected = get_test_tensors(N_inp, N_out, d)\n",
    "    O_all_smem = torch.zeros(N_out, d, device=\"cuda\").contiguous()\n",
    "    L_all_smem = torch.zeros(N_out, device=\"cuda\")\n",
    "    tpb = (block_dim_x, block_dim_y)\n",
    "    grid = (1,)\n",
    "    flash_attention_numba_all_smem[grid, tpb](Q, K, V, scaling, L_all_smem, O_all_smem,  N_out, N_inp)\n",
    "    torch.cuda.synchronize()\n",
    "    check_close(\n",
    "        O_all_smem, \n",
    "        O_expected,\n",
    "        L_all_smem,\n",
    "        L_expected,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving `m_i`, `l_i`, `O_i` to local arrays\n",
    "\n",
    "Current shared-memory usage across threads:\n",
    "```\n",
    "Shar = (B_r * d * 4) # Q_i\n",
    "+ (B_c * d * 4) # K_j\n",
    "+ (B_c * d * 4) # V_j\n",
    "+ (B_r * B_c * 4) # S\n",
    "= ~25 KB\n",
    "```\n",
    "\n",
    "Current block-shared accumulators (`m_i`, `l_i`, `O_i`):\n",
    "```\n",
    "Loc = 4 * (B_r + B_r + (B_r * d)) = 8320 B ≈ 8 KB\n",
    "```\n",
    "\n",
    "Total shared usage: **~33 KB** (fine for 1 block/SM).\n",
    "\n",
    "---\n",
    "\n",
    "**Idea:** Move `m_i`, `l_i`, `O_i` to *per-thread* locals to fit in registers.\n",
    "\n",
    "Problem: Full-size per-thread arrays would need\n",
    "\n",
    "```\n",
    "Loc * 32 * 16 ≈ 4 MB > 64 KB register file per SM\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Optimization:** With tiling `tpb = (32, 16)`:\n",
    "\n",
    "- Each thread handles only  \n",
    "  `d // blockDim.x = 4` columns in `x`  \n",
    "  `B_r // blockDim.y = 1` row in `y`\n",
    "- So per-thread locals can be much smaller:\n",
    "\n",
    "```python\n",
    "l_i = numba.cuda.local.array((1,), inp_dtype)   # 4 B\n",
    "m_i = numba.cuda.local.array((1,), inp_dtype)   # 4 B\n",
    "O_i = numba.cuda.local.array((4,), inp_dtype)   # 16 B\n",
    "```\n",
    "-> Per-thread = 24 B, per block = 24 * 32 * 16 = 12 KB < 64 KB -> avoid register pressure and spills.\n",
    "\n",
    "\n",
    "This is how we set up `flash_attention_numba` below and the cuda version in `./flash_attention.cu`\n",
    "\n",
    "In the performance section we run `./flash_attention_spilling_from_registers.cu` that fits full arrays in local variables, to show the performance decrease by slowing the kernel by ~2.5×"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_dim_x = 32\n",
    "block_dim_y = 16\n",
    "B_r = 16\n",
    "B_c = 16\n",
    "d_over_dim_x = d // block_dim_x\n",
    "B_r_over_dim_y = B_r // block_dim_y\n",
    "\n",
    "@numba.cuda.jit\n",
    "def flash_attention_numba(Q, K, V, scaling: numba.float32, L, O, N_out, N_inp):\n",
    "    B_c = 16\n",
    "    B_r = 16\n",
    "    T_c = (N_inp + B_c - 1) // B_c\n",
    "    T_r = (N_out + B_r - 1) // B_r\n",
    "    inp_dtype = K.dtype\n",
    "    tid_x = numba.cuda.threadIdx.x\n",
    "    tid_y = numba.cuda.threadIdx.y\n",
    "    dim_y = numba.cuda.blockDim.y\n",
    "    dim_x = numba.cuda.blockDim.x\n",
    "    \n",
    "\n",
    "    Q_i = numba.cuda.shared.array((B_r, d), inp_dtype)\n",
    "    K_j = numba.cuda.shared.array((B_c, d), inp_dtype)\n",
    "    V_j = numba.cuda.shared.array((B_c, d), inp_dtype)\n",
    "    S = numba.cuda.shared.array((B_r, B_c), inp_dtype)\n",
    "\n",
    "    l_i = numba.cuda.local.array((B_r_over_dim_y,), inp_dtype)\n",
    "    m_i = numba.cuda.local.array((B_r_over_dim_y,), inp_dtype)\n",
    "    O_i = numba.cuda.local.array((B_r_over_dim_y, d_over_dim_x), inp_dtype)\n",
    "\n",
    "                 \n",
    "    for i in range(T_r):\n",
    "        for ii in range(tid_y, B_r, dim_y):\n",
    "            for dd in range(tid_x, d, dim_x):\n",
    "                Q_i[ii, dd] = Q[ii + i * B_r, dd]\n",
    "        numba.cuda.syncthreads()\n",
    "\n",
    "        for ii in range(B_r_over_dim_y):\n",
    "            for dd in range(d_over_dim_x):\n",
    "                O_i[ii, dd] = 0\n",
    "            l_i[ii] = 0\n",
    "            m_i[ii] = -math.inf\n",
    "        numba.cuda.syncthreads()\n",
    "\n",
    "        for j in range(T_c):\n",
    "            for jj in range(tid_y, B_c, dim_y):\n",
    "                for dd in range(tid_x, d, dim_x):\n",
    "                    K_j[jj, dd] = K[jj + j * B_c, dd]\n",
    "                    V_j[jj, dd] = V[jj + j * B_c, dd]\n",
    "\n",
    "            # S[ii][jj] = scaling * (Q_i @ K_j.T)\n",
    "            numba.cuda.syncthreads()\n",
    "            for ii in range(tid_x, B_r, dim_x):\n",
    "                for jj in range(tid_y, B_c, dim_y):\n",
    "                    S_ij = 0\n",
    "                    for dd in range(d):\n",
    "                        S_ij += Q_i[ii, dd] * K_j[jj, dd]\n",
    "                    S_ij = scaling * S_ij\n",
    "                    S[ii][jj] = S_ij\n",
    "\n",
    "            numba.cuda.syncthreads()\n",
    "            for ii in range(B_r_over_dim_y):\n",
    "                m = m_i[ii]\n",
    "                last_m = m\n",
    "                for jj in range(B_c):\n",
    "                    m = max(m, S[ii * dim_y + tid_y][jj])\n",
    "                m_i[ii] = m\n",
    "                l = numba.float32(math.exp(last_m - m)) * l_i[ii]\n",
    "\n",
    "                for dd in range(d_over_dim_x):\n",
    "                    O_i[ii, dd] *= numba.float32(math.exp(last_m - m))\n",
    "                for jj in range(B_c):\n",
    "                    P_ij = numba.float32(math.exp(S[ii * dim_y + tid_y][jj] - m))\n",
    "                    l += P_ij\n",
    "                    for dd in range(d_over_dim_x):\n",
    "                        O_i[ii, dd] += P_ij * V_j[jj, dd * dim_x + tid_x]\n",
    "                l_i[ii] = l\n",
    "                \n",
    "        numba.cuda.syncthreads()\n",
    "        for ii in range(B_r_over_dim_y):  \n",
    "            for dd in range(d_over_dim_x):\n",
    "                O[ii * dim_y + tid_y + i * B_r, dd * dim_x + tid_x] = O_i[ii, dd] / l_i[ii]\n",
    "            L[ii * dim_y + tid_y + i * B_r] = m_i[ii] + numba.float32(math.log(l_i[ii]))\n",
    "        numba.cuda.syncthreads() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max absolute difference:\n",
      "O:  tensor(4.1723e-07, device='cuda:0')\n",
      "L:  tensor(4.7684e-07, device='cuda:0')\n",
      "Max absolute difference:\n",
      "O:  tensor(2.9802e-07, device='cuda:0')\n",
      "L:  tensor(4.7684e-07, device='cuda:0')\n",
      "Max absolute difference:\n",
      "O:  tensor(4.1723e-07, device='cuda:0')\n",
      "L:  tensor(9.5367e-07, device='cuda:0')\n",
      "Max absolute difference:\n",
      "O:  tensor(1.1921e-06, device='cuda:0')\n",
      "L:  tensor(1.4305e-06, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for N_inp, N_out in TEST_DIMS:\n",
    "    Q, K, V, scaling, O_expected, L_expected = get_test_tensors(N_inp, N_out, d)\n",
    "\n",
    "    O_all_smem = torch.zeros(N_out, d, device=\"cuda\").contiguous()\n",
    "    L_all_smem = torch.zeros(N_out, device=\"cuda\")\n",
    "    tpb = (block_dim_x, block_dim_y)\n",
    "    grid = (1,)\n",
    "    flash_attention_numba[grid, tpb](Q, K, V, scaling, L_all_smem, O_all_smem,  N_out, N_inp)\n",
    "    check_close(\n",
    "        O_all_smem, \n",
    "        O_expected,\n",
    "        L_all_smem,\n",
    "        L_expected,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuda\n",
    "\n",
    "### flash_attention_numba in Cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/zeus/.cache/torch_extensions/py310_cu128 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/zeus/.cache/torch_extensions/py310_cu128/flash_attentiontest/build.ninja...\n",
      "Building extension module flash_attentiontest...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module flash_attentiontest...\n"
     ]
    }
   ],
   "source": [
    "fname = \"flash_attention\"\n",
    "module_cuda = get_loaded_cuda_module(fname, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max absolute difference:\n",
      "O:  tensor(8.9407e-07, device='cuda:0')\n",
      "L:  tensor(4.7684e-07, device='cuda:0')\n",
      "Max absolute difference:\n",
      "O:  tensor(1.0133e-06, device='cuda:0')\n",
      "L:  tensor(9.5367e-07, device='cuda:0')\n",
      "Max absolute difference:\n",
      "O:  tensor(1.4007e-06, device='cuda:0')\n",
      "L:  tensor(9.5367e-07, device='cuda:0')\n",
      "Max absolute difference:\n",
      "O:  tensor(7.3016e-07, device='cuda:0')\n",
      "L:  tensor(1.4305e-06, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for N_inp, N_out in TEST_DIMS:\n",
    "\n",
    "    Q, K, V, _, O_expected, L_expected = get_test_tensors(N_inp, N_out, d)\n",
    "    O_move_registers, L_move_registers = getattr(module_cuda, fname)(Q, K, V)\n",
    "    check_close(\n",
    "        O_move_registers, \n",
    "        O_expected,\n",
    "        L_move_registers,\n",
    "        L_expected,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuda-Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvrtcResult.NVRTC_SUCCESS\n",
      "nvrtcResult.NVRTC_SUCCESS\n",
      "\u0000\n",
      "CUresult.CUDA_SUCCESS\n",
      "CUresult.CUDA_SUCCESS <CUfunction 0x3083c470>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([700605696, 700605704, 700605712, 700605720, 700605728, 815044608,\n",
       "        814450368, 814450372, 814450376], dtype=torch.uint64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create program\n",
    "# conda install conda-forge::cuda-python\n",
    "from cuda import cuda, nvrtc\n",
    "\n",
    "cuda_src_path = f\"./flash_attention.cu\"\n",
    "cuda_src = Path(cuda_src_path).read_text()\n",
    "\n",
    "N_inp = 32\n",
    "N_out = 32\n",
    "d = 128\n",
    "B_r, B_c = 16, 16\n",
    "T_r = (N_out + B_r -1) // B_r\n",
    "T_c = (N_inp + B_r -1) // B_c\n",
    "Q, K, V, scale_factor, O_expected, L_expected = get_test_tensors(N_inp, N_out, d)\n",
    "\n",
    "err, prog = nvrtc.nvrtcCreateProgram(str.encode(cuda_src), b\"flash_attention.cu\", 0, [], [])\n",
    "\n",
    "# Compile program\n",
    "min, maj = torch.cuda.get_device_capability()\n",
    "opts = [\n",
    "    f\"--gpu-architecture=compute_{min}{maj}\".encode(), \n",
    "    \"--device-as-default-execution-space\".encode(),\n",
    "    \"--std=c++14\".encode()]\n",
    "err, = nvrtc.nvrtcCompileProgram(prog, len(opts), opts)\n",
    "\n",
    "print(err)\n",
    "\n",
    "# Get PTX from compilation\n",
    "err, ptxSize = nvrtc.nvrtcGetPTXSize(prog)\n",
    "ptx = b\" \" * ptxSize\n",
    "err, = nvrtc.nvrtcGetPTX(prog, ptx)\n",
    "print(err)\n",
    "\n",
    "err, logSize = nvrtc.nvrtcGetProgramLogSize(prog)\n",
    "log = b\" \" * logSize\n",
    "err, = nvrtc.nvrtcGetProgramLog(prog, log)\n",
    "print(log.decode())\n",
    "# print(ptx.decode())\n",
    "\n",
    "# Load PTX as module data and retrieve function\n",
    "err, module = cuda.cuModuleLoadData(ptx)\n",
    "print(err)\n",
    "err, kernel = cuda.cuModuleGetFunction(module, b\"flash_attention_k\")\n",
    "print(err, kernel)\n",
    "\n",
    "# Allocate tensors\n",
    "# S3 = torch.zeros(N_out, N_out, device=\"cuda\")\n",
    "O_cuda_py = torch.zeros(N_out, d, device=\"cuda\")\n",
    "L_cuda_py = torch.zeros(N_out, device=\"cuda\")\n",
    "\n",
    "# To quote the official tutorial: (https://nvidia.github.io/cuda-python/overview.html)\n",
    "# The following code example is not intuitive\n",
    "# Subject to change in a future release\n",
    "\n",
    "int_args = torch.tensor([0, T_r, T_c], dtype=torch.int32)\n",
    "float_args = torch.tensor([scale_factor], dtype=torch.float32)\n",
    "ptr_args = torch.tensor([i.data_ptr() for i in (O_cuda_py, L_cuda_py, Q, K, V)], dtype=torch.uint64)\n",
    "\n",
    "args = torch.tensor([\n",
    "    *(i.data_ptr() for i in ptr_args),\n",
    "    *(i.data_ptr() for i in float_args),\n",
    "    *(i.data_ptr() for i in int_args)], dtype=torch.uint64)\n",
    "\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1325e-06, device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fn():\n",
    "    err = cuda.cuLaunchKernel(\n",
    "        kernel,\n",
    "        1,  # grid x dim\n",
    "        1,  # grid y dim\n",
    "        1,  # grid z dim\n",
    "        16,  # block x dim\n",
    "        16,  # block y dim\n",
    "        1,  # block z dim\n",
    "        0,  # dynamic shared memory\n",
    "        torch.cuda.current_stream().stream_id,  # stream\n",
    "        args.data_ptr(),  # kernel arguments\n",
    "        0,  # extra (ignore)\n",
    "    )\n",
    "\n",
    "fn()\n",
    "\n",
    "(O_cuda_py - O_expected).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "**********\n",
      "Dimensions: N_out=32, N_inp=32, d=128\n",
      "\n",
      "- Custom Flash Attention: Cuda-python\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 µs ± 1.05 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n\\n**********\\nDimensions: {N_out=}, {N_inp=}, {d=}\")\n",
    "torch.cuda.synchronize()\n",
    "print(\"\\n- Custom Flash Attention: Cuda-python\")\n",
    "%timeit fn(); torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance \n",
    "\n",
    "Run timeit on different dimensions. \n",
    "\n",
    "- Recall that we build the kernel for `d=128` and design it so that it computes the full attention in a single block.\n",
    "\n",
    "- For matrices with small `N_out` this implementation is significantly faster than `scaled_dot_product_attention`.\n",
    "\n",
    "- But by making N larger this implementation slows down dramatically it only uses a single block.\n",
    "\n",
    "- Note that the register splilling version is much slower even compared to `scaled_dot_product_attention`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load registers spilling version\n",
    "\n",
    "This is the version loading full arrays as local variables in threads, which leads to spilling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max absolute difference:\n",
      "O:  tensor(1.1325e-06, device='cuda:0')\n",
      "L:  tensor(4.7684e-07, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "fname_spill_from_registers = \"flash_attention_spilling_from_registers\"\n",
    "module_cuda_spilling_from_registers = get_loaded_cuda_module(fname_spill_from_registers)\n",
    "O_cuda_spilling, L_cuda_spilling = getattr(module_cuda_spilling_from_registers, fname_spill_from_registers)(Q, K, V)\n",
    "check_close(O_cuda_spilling, O_expected, L_cuda_spilling, L_expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "**********\n",
      "Dimensions: N_out=32, N_inp=32, d=128\n",
      "\n",
      "- Torch scaled_dot_product_attention\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176 µs ± 2.54 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "\n",
      "- Custom Flash Attention\n",
      "88.5 µs ± 901 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "\n",
      "- Custom Flash Attention: spill from registers\n",
      "353 µs ± 1.73 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "\n",
      "\n",
      "**********\n",
      "Dimensions: N_out=64, N_inp=128, d=128\n",
      "\n",
      "- Torch scaled_dot_product_attention\n",
      "177 µs ± 3.12 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "\n",
      "- Custom Flash Attention\n",
      "337 µs ± 958 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "\n",
      "- Custom Flash Attention: spill from registers\n",
      "2.41 ms ± 4.43 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "\n",
      "**********\n",
      "Dimensions: N_out=512, N_inp=512, d=128\n",
      "\n",
      "- Torch scaled_dot_product_attention\n",
      "207 µs ± 2.47 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "\n",
      "- Custom Flash Attention\n",
      "9.02 ms ± 6.53 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "- Custom Flash Attention: spill from registers\n",
      "74.8 ms ± 21.9 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "\n",
      "**********\n",
      "Dimensions: N_out=1024, N_inp=512, d=128\n",
      "\n",
      "- Torch scaled_dot_product_attention\n",
      "273 µs ± 4.05 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "\n",
      "- Custom Flash Attention\n",
      "18 ms ± 4.36 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "- Custom Flash Attention: spill from registers\n",
      "149 ms ± 23.1 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "for N_inp, N_out in TEST_DIMS:\n",
    "    Q, K, V, _, _, _ = get_test_tensors(N_inp, N_out, d)\n",
    "    print(f\"\\n\\n**********\\nDimensions: {N_out=}, {N_inp=}, {d=}\")\n",
    "    torch.cuda.synchronize()\n",
    "    print(\"\\n- Torch scaled_dot_product_attention\")\n",
    "    %timeit torch.nn.functional.scaled_dot_product_attention(Q, K, V); torch.cuda.synchronize()\n",
    "    print(\"\\n- Custom Flash Attention\")\n",
    "    %timeit getattr(module_cuda, fname)(Q, K, V); torch.cuda.synchronize()\n",
    "    print(\"\\n- Custom Flash Attention: spill from registers\")\n",
    "    %timeit getattr(module_cuda_spilling_from_registers, fname_spill_from_registers)(Q, K, V); torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profile\n",
    "\n",
    "We can compare the performance of the kernel with and without register spilling as follows:\n",
    "```\n",
    "# Get ptx files\n",
    "nvcc -arch=sm_80 -ptx flash_attention.cu -o flash_attention.ptx\n",
    "nvcc -arch=sm_80 -ptx flash_attention_spilling_from_registers.cu -o flash_attention_spilling_from_registers.ptx\n",
    "\n",
    "# Get ncu metrics\n",
    "nvcc -O3 -o test_attention main.cu flash_attention.cu flash_attention_spilling_from_registers.cu\n",
    "ncu ./test_attention\n",
    "```\n",
    "\n",
    "### PTX comparison\n",
    "\n",
    "TODO\n",
    "\n",
    "\n",
    "### Nsight Compute Comparison\n",
    "\n",
    "| **Metric** | **Spilling Kernel (`flash_attention_spilling_from_registers_k`)** | **Non-Spilling Kernel (`flash_attention_k`)** | **Difference / Interpretation** |\n",
    "|------------|------------------------------------------------------------------|-----------------------------------------------|---------------------------------|\n",
    "| **Duration** | **13.02 ms** | **2.10 ms** | Spilling kernel is ~6× slower. |\n",
    "| **Elapsed Cycles** | 7.6 M | 1.2 M | Extra cycles lost to spills. |\n",
    "| **Compute (SM) Throughput** | **0.26%** | **1.20%** | 5× higher compute utilization in non-spilling kernel. |\n",
    "| **L2 Cache Throughput** | **1.71%** | **0.56%** | Spilling kernel hits L2 more — spilled registers go to local memory via L2. |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "flash_attention_spilling_from_registers_k (1, 1, 1)x(32, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
    "    Section: GPU Speed Of Light Throughput\n",
    "    ----------------------- ------------- ------------\n",
    "    Metric Name               Metric Unit Metric Value\n",
    "    ----------------------- ------------- ------------\n",
    "    DRAM Frequency          cycle/nsecond         5.00\n",
    "    SM Frequency            cycle/usecond       585.15\n",
    "    Elapsed Cycles                  cycle      7619363\n",
    "    Memory Throughput                   %         1.86\n",
    "    DRAM Throughput                     %         0.01\n",
    "    Duration                      msecond        13.02\n",
    "    L1/TEX Cache Throughput             %        74.55\n",
    "    L2 Cache Throughput                 %         1.71\n",
    "    SM Active Cycles                cycle    190430.67\n",
    "    Compute (SM) Throughput             %         0.26\n",
    "    ----------------------- ------------- ------------\n",
    "\n",
    "    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
    "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
    "\n",
    "    Section: Launch Statistics\n",
    "    -------------------------------- --------------- ---------------\n",
    "    Metric Name                          Metric Unit    Metric Value\n",
    "    -------------------------------- --------------- ---------------\n",
    "    Block Size                                                   512\n",
    "    Function Cache Configuration                     CachePreferNone\n",
    "    Grid Size                                                      1\n",
    "    Registers Per Thread             register/thread              60\n",
    "    Shared Memory Configuration Size           Kbyte           65.54\n",
    "    Driver Shared Memory Per Block        byte/block               0\n",
    "    Dynamic Shared Memory Per Block       byte/block               0\n",
    "    Static Shared Memory Per Block       Kbyte/block           25.60\n",
    "    Threads                                   thread             512\n",
    "    Waves Per SM                                                0.01\n",
    "    -------------------------------- --------------- ---------------\n",
    "\n",
    "    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
    "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
    "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
    "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
    "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
    "          description for more details on launch configurations.                                                        \n",
    "\n",
    "    Section: Occupancy\n",
    "    ------------------------------- ----------- ------------\n",
    "    Metric Name                     Metric Unit Metric Value\n",
    "    ------------------------------- ----------- ------------\n",
    "    Block Limit SM                        block           16\n",
    "    Block Limit Registers                 block            2\n",
    "    Block Limit Shared Mem                block            2\n",
    "    Block Limit Warps                     block            2\n",
    "    Theoretical Active Warps per SM        warp           32\n",
    "    Theoretical Occupancy                     %          100\n",
    "    Achieved Occupancy                        %        50.00\n",
    "    Achieved Active Warps Per SM           warp        16.00\n",
    "    ------------------------------- ----------- ------------\n",
    "\n",
    "    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     \n",
    "          theoretical (100.0%) and measured achieved occupancy (50.0%) can be the result of warp scheduling overheads   \n",
    "          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    \n",
    "          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                \n",
    "          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n",
    "          optimizing occupancy.                                                                                         \n",
    "\n",
    "  flash_attention_k (1, 1, 1)x(32, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
    "    Section: GPU Speed Of Light Throughput\n",
    "    ----------------------- ------------- ------------\n",
    "    Metric Name               Metric Unit Metric Value\n",
    "    ----------------------- ------------- ------------\n",
    "    DRAM Frequency          cycle/nsecond         4.99\n",
    "    SM Frequency            cycle/usecond       584.08\n",
    "    Elapsed Cycles                  cycle      1225715\n",
    "    Memory Throughput                   %         1.72\n",
    "    DRAM Throughput                     %         0.04\n",
    "    Duration                      msecond         2.10\n",
    "    L1/TEX Cache Throughput             %        68.85\n",
    "    L2 Cache Throughput                 %         0.56\n",
    "    SM Active Cycles                cycle     30658.20\n",
    "    Compute (SM) Throughput             %         1.20\n",
    "    ----------------------- ------------- ------------\n",
    "\n",
    "    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
    "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
    "\n",
    "    Section: Launch Statistics\n",
    "    -------------------------------- --------------- ---------------\n",
    "    Metric Name                          Metric Unit    Metric Value\n",
    "    -------------------------------- --------------- ---------------\n",
    "    Block Size                                                   512\n",
    "    Function Cache Configuration                     CachePreferNone\n",
    "    Grid Size                                                      1\n",
    "    Registers Per Thread             register/thread              62\n",
    "    Shared Memory Configuration Size           Kbyte           65.54\n",
    "    Driver Shared Memory Per Block        byte/block               0\n",
    "    Dynamic Shared Memory Per Block       byte/block               0\n",
    "    Static Shared Memory Per Block       Kbyte/block           25.60\n",
    "    Threads                                   thread             512\n",
    "    Waves Per SM                                                0.01\n",
    "    -------------------------------- --------------- ---------------\n",
    "\n",
    "    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
    "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
    "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
    "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
    "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
    "          description for more details on launch configurations.                                                        \n",
    "\n",
    "    Section: Occupancy\n",
    "    ------------------------------- ----------- ------------\n",
    "    Metric Name                     Metric Unit Metric Value\n",
    "    ------------------------------- ----------- ------------\n",
    "    Block Limit SM                        block           16\n",
    "    Block Limit Registers                 block            2\n",
    "    Block Limit Shared Mem                block            2\n",
    "    Block Limit Warps                     block            2\n",
    "    Theoretical Active Warps per SM        warp           32\n",
    "    Theoretical Occupancy                     %          100\n",
    "    Achieved Occupancy                        %        50.00\n",
    "    Achieved Active Warps Per SM           warp        16.00\n",
    "    ------------------------------- ----------- ------------\n",
    "\n",
    "    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     \n",
    "          theoretical (100.0%) and measured achieved occupancy (50.0%) can be the result of warp scheduling overheads   \n",
    "          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    \n",
    "          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                \n",
    "          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n",
    "          optimizing occupancy.                                                                                         \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thunder\n",
    "\n",
    "[Installation guide](https://lightning.ai/docs/thunder/latest/fundamentals/installation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import thunder\n",
    "\n",
    "attn_ex = thunder.extend.OperatorExecutor('attn_ex', version=0.01)\n",
    "thunder.add_default_executor(attn_ex)\n",
    "\n",
    "# [attn_ex, attn_ex, sdpa, nvfuser]\n",
    "\n",
    "def my_attn_impl(query, key, value, scale):\n",
    "    n_out, d = query.shape\n",
    "\n",
    "    # S3 = torch.zeros(N_out, N_out, device=\"cuda\")\n",
    "    O3 = torch.zeros(N_out, d, device=\"cuda\")\n",
    "    L3 = torch.zeros(N_out, device=\"cuda\")\n",
    "\n",
    "    B_c = 16\n",
    "    B_r = 16\n",
    "    T_c = (N_inp + B_c - 1) // B_c\n",
    "    T_r = (N_out + B_r - 1) // B_r\n",
    "\n",
    "    int_args = torch.tensor([N_out, T_r, T_c], dtype=torch.int32)\n",
    "    float_args = torch.tensor([scale_factor], dtype=torch.float32)\n",
    "    ptr_args = torch.tensor([i.data_ptr() for i in (O3, L3, key, query, value)], dtype=torch.uint64)\n",
    "\n",
    "    args = torch.tensor([\n",
    "        *(i.data_ptr() for i in ptr_args),\n",
    "        *(i.data_ptr() for i in float_args),\n",
    "        *(i.data_ptr() for i in int_args)], dtype=torch.uint64\n",
    "    )\n",
    "\n",
    "    err, _ = cuda.cuLaunchKernel(\n",
    "        kernel,\n",
    "        1,  # grid x dim\n",
    "        1,  # grid y dim\n",
    "        1,  # grid z dim\n",
    "        32, # block x dim\n",
    "        32, # block y dim\n",
    "        1,  # block z dim\n",
    "        0,  # dynamic shared memory\n",
    "        torch.cuda.current_stream().stream_id,  # stream\n",
    "        args.data_ptr(),  # kernel arguments\n",
    "        0,  # extra (ignore)\n",
    "    )\n",
    "    assert err == cuda.CUresult.CUDA_SUCCESS, err\n",
    "    return O3, L3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Register our implementation as an operator\n",
    "def my_attn_meta(query, key, value, scale):\n",
    "    return thunder.TensorProxy(like=query), thunder.TensorProxy(like=query, shape=(query.shape[:-1],))\n",
    "\n",
    "my_attn = attn_ex.register_operator('my_attn', meta=my_attn_meta, fn=my_attn_impl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_attn_checker(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False, scale=None):\n",
    "    if attn_mask is not None or dropout_p == 0.0 or is_causal:\n",
    "        return False\n",
    "    if len(query.shape) > 2:\n",
    "            return (query.device.device_type == thunder.devices.DeviceType.CUDA and\n",
    "                key.device == query.device and\n",
    "                value.device == query.device)\n",
    "    return False\n",
    "\n",
    "def my_attn_transform(query, key, value, attn_masks=None, dropout_p=0.0, is_causal=False, scale=None):\n",
    "    if scale is None:\n",
    "        scale = query.size(-1) ** -0.5\n",
    "    out = my_attn(query, key, value, scale)\n",
    "    return out[0]\n",
    "\n",
    "attn_ex.register_implementation(thunder.torch.scaled_dot_product_attention, checker=my_attn_checker,\n",
    "                                  execution_transform=my_attn_transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., device='cuda:0')\n",
      "# Constructed by Unwrap the actual return value\n",
      "import torch\n",
      "import torch.nn.functional\n",
      "from thunder.executors.torchex import no_autocast\n",
      "\n",
      "@torch.no_grad()\n",
      "@no_autocast\n",
      "def computation(query, key, value):\n",
      "  # query: \"cuda:0 f32[1024, 128]\"\n",
      "  # key: \"cuda:0 f32[512, 128]\"\n",
      "  # value: \"cuda:0 f32[512, 128]\"\n",
      "\n",
      "  # /tmp/ipykernel_30370/3826419770.py:2: \t        return torch.nn.functional.scaled_dot_product_attention(query, key, value, is_causal=False)\n",
      "  t41 = torch.nn.functional.scaled_dot_product_attention(query, key, value, None, 0.0, False, scale=None)  # t41: \"cuda:0 f32[1024, 128]\"\n",
      "    # t41 = ltorch.scaled_dot_product_attention(query, key, value, None, 0.0, False, scale=None)  # t41: \"cuda:0 f32[1024, 128]\"\n",
      "      # t28 = ltorch.mul(query, 0.29730177875068026)  # t28: \"cuda:0 f32[1024, 128]\"\n",
      "        # t28 = prims.mul(query, 0.29730177875068026)  # t28: \"cuda:0 f32[1024, 128]\"\n",
      "      # t29 = ltorch.transpose(key, -2, -1)  # t29: \"cuda:0 f32[128, 512]\"\n",
      "        # t29 = prims.transpose(key, (1, 0))  # t29: \"cuda:0 f32[128, 512]\"\n",
      "      # t30 = ltorch.mul(t29, 0.29730177875068026)  # t30: \"cuda:0 f32[128, 512]\"\n",
      "        # t30 = prims.mul(t29, 0.29730177875068026)  # t30: \"cuda:0 f32[128, 512]\"\n",
      "      # t31 = ltorch.matmul(t28, t30)  # t31: \"cuda:0 f32[1024, 512]\"\n",
      "        # t31 = prims.matmul(t28, t30)  # t31: \"cuda:0 f32[1024, 512]\"\n",
      "      # t40 = ltorch._softmax(t31, -1, dtype=None)  # t40: \"cuda:0 f32[1024, 512]\"\n",
      "        # t33 = ltorch.amax(t31, -1, True)  # t33: \"cuda:0 f32[1024, 1]\"\n",
      "          # t32 = prims.amax(t31, (1,))  # t32: \"cuda:0 f32[1024]\"\n",
      "          # t33 = prims.broadcast_in_dim(t32, [1024, 1], [0])  # t33: \"cuda:0 f32[1024, 1]\"\n",
      "        # t35 = ltorch.sub(t31, t33, alpha=1)  # t35: \"cuda:0 f32[1024, 512]\"\n",
      "          # t34 = prims.broadcast_in_dim(t33, (1024, 512), (0, 1))  # t34: \"cuda:0 f32[1024, 512]\"\n",
      "          # t35 = prims.sub(t31, t34)  # t35: \"cuda:0 f32[1024, 512]\"\n",
      "        # t36 = ltorch.exp(t35)  # t36: \"cuda:0 f32[1024, 512]\"\n",
      "          # t36 = prims.exp(t35)  # t36: \"cuda:0 f32[1024, 512]\"\n",
      "        # t38 = ltorch.sum(t36, -1, True, dtype=None)  # t38: \"cuda:0 f32[1024, 1]\"\n",
      "          # t37 = prims.sum(t36, (1,))  # t37: \"cuda:0 f32[1024]\"\n",
      "          # t38 = prims.broadcast_in_dim(t37, [1024, 1], [0])  # t38: \"cuda:0 f32[1024, 1]\"\n",
      "        # t40 = ltorch.true_divide(t36, t38)  # t40: \"cuda:0 f32[1024, 512]\"\n",
      "          # t39 = prims.broadcast_in_dim(t38, (1024, 512), (0, 1))  # t39: \"cuda:0 f32[1024, 512]\"\n",
      "          # t40 = prims.div(t36, t39)  # t40: \"cuda:0 f32[1024, 512]\"\n",
      "      # t41 = ltorch.matmul(t40, value)  # t41: \"cuda:0 f32[1024, 128]\"\n",
      "        # t41 = prims.matmul(t40, value)  # t41: \"cuda:0 f32[1024, 128]\"\n",
      "  return (t41,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def test_fn(query, key, value):\n",
    "        return torch.nn.functional.scaled_dot_product_attention(query, key, value, is_causal=False)\n",
    "\n",
    "jfn = thunder.jit(test_fn)\n",
    "\n",
    "print((jfn(Q, K, V) - test_fn(Q, K, V)).abs().max())\n",
    "print(thunder.last_traces(jfn)[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuda info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PyTorch CUDA Info ===\n",
      "PyTorch version: 2.7.1+cu128\n",
      "CUDA available: True\n",
      "CUDA version: 12.8\n",
      "cuDNN version: 90701\n",
      "Number of GPUs: 1\n",
      "  GPU 0: Tesla T4\n",
      "    Current device: 0\n",
      "    Memory allocated: 10.77 MB\n",
      "    Memory cached   : 27.26 MB\n",
      "\n",
      "=== nvidia-smi Info (if available) ===\n",
      "Mon Aug 18 10:30:31 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.261.03             Driver Version: 535.261.03   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       Off | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   42C    P0              34W /  70W |    917MiB / 15360MiB |     64%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "print_cuda_info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
