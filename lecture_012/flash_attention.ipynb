{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flash Attention in Torch, Numba and Cuda\n",
    "\n",
    "\n",
    "We implement in 3 different ways the forward algorithm from the [Flash Attention 2 paper](https://arxiv.org/pdf/2307.08691):\n",
    "\n",
    "1. Torch operations\n",
    "2. Numba\n",
    "3. Cuda\n",
    "\n",
    "We do some basic performance analysis as well as running the custom kernel with cuda-python and thunder.\n",
    "\n",
    "\n",
    "- We build the kernel for `d=128` and design it so that it computes the full attention in a single block.\n",
    "\n",
    "![./flash_attention_fwd.png](./flash_attention_fwd.png)\n",
    "\n",
    "\n",
    "## Utils\n",
    "\n",
    "todo: \n",
    "- show spills and investigate why larger matrices slower with profiler? allegedly it becoems comput intensive as we use only 1 block, prove that?\n",
    "- mention no thunder example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/scipy/__init__.py:132: UserWarning: A NumPy version >=1.21.6 and <1.28.0 is required for this version of SciPy (detected version 2.0.0)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numba\n",
    "from numba.cuda import as_cuda_array as ca\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import sys, os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "from utils import load_cuda, get_sig, print_cuda_info\n",
    "\n",
    "import os\n",
    "\n",
    "def get_loaded_cuda_module(fname, verbose=False):\n",
    "    cuda_src_path = f\"./{fname}.cu\"\n",
    "    torch_src_path = f\"./torch_extension_template.cu\"\n",
    "    cuda_src = Path(cuda_src_path).read_text()\n",
    "    cuda_src += Path(torch_src_path).read_text()\n",
    "    cuda_src = cuda_src.replace(\"your_function_name\", fname)\n",
    "    cpp_src = get_sig(fname, cuda_src)\n",
    "    return load_cuda(cuda_src, cpp_src, [fname], verbose=verbose)\n",
    "\n",
    "\n",
    "def check_diff(O, O_expected, L=None, L_expected=None, atol=5*1e-5):\n",
    "    O_diff = (O-O_expected).abs().max()\n",
    "    print(\"Max absolute difference:\")\n",
    "    if atol:\n",
    "        assert O_diff < atol, f\"O diff too large: {O_diff} > {atol=}\"\n",
    "    print(\"O: \", O_diff)\n",
    "    if L is not None:\n",
    "        L_diff = (L.squeeze()-L_expected).abs().max()\n",
    "        if atol:\n",
    "            assert L_diff < atol, f\"L diff too large: {L_diff} > {atol=}\"\n",
    "        print(\"L: \", L_diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max absolute difference:\n",
      "O:  tensor(4.4703e-07, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Test tensors\n",
    "def get_test_tensors(N_inp, N_out, d):\n",
    "    Q = torch.randn(N_out, d).contiguous().to(\"cuda\")\n",
    "    K = torch.randn(N_inp, d).contiguous().to(\"cuda\")\n",
    "    V = torch.randn(N_inp, d).contiguous().to(\"cuda\")\n",
    "    scaling = 1.0 / math.sqrt(d)\n",
    "\n",
    "    # Get expected O\n",
    "    O_expected = torch.nn.functional.scaled_dot_product_attention(Q, K, V)\n",
    "    S = (Q @ K.T) * scaling  # shape: (N_out, N_inp)\n",
    "    L_expected = torch.logsumexp(S, dim=-1)\n",
    "    return Q, K, V, scaling, O_expected, L_expected\n",
    "\n",
    "N_inp = 512\n",
    "N_out = 512\n",
    "d = 128\n",
    "\n",
    "Q, K, V, scaling, O_expected, L_expected = get_test_tensors(N_inp, N_out, d)\n",
    "check_diff(O=torch.softmax(Q @ K.T * scaling, dim=-1) @ V, O_expected=O_expected)\n",
    "\n",
    "TEST_DIMS = [\n",
    "    (32, 32),\n",
    "    (512, 512),\n",
    "    (128, 64),\n",
    "    (512, 1024),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flash_attention_torch(Q, K, V, O, L, N_inp, N_out, d) -> None:\n",
    "    \"\"\"Forward algo from https://arxiv.org/pdf/2307.08691\n",
    "    \"\"\"\n",
    "\n",
    "    B_c = 16\n",
    "    B_r = 16\n",
    "    T_c = (N_inp + B_c - 1) // B_c\n",
    "    T_r = (N_out + B_r - 1) // B_r\n",
    "\n",
    "    scaling = 1 / math.sqrt(d)\n",
    "\n",
    "    # Q and O L split into T_r; K, V in T_c blocks\n",
    "    for i in range(T_r):\n",
    "        Q_i = Q[i * B_r : (i + 1) * B_r]\n",
    "        O_i = torch.zeros(B_r, d)\n",
    "        L_i = torch.zeros(B_r, 1)\n",
    "        m_i = torch.full((B_r, 1), -math.inf)\n",
    "        last_m_i = m_i\n",
    "        for j in range(T_c):\n",
    "            K_j = K[j * B_c : (j + 1) * B_c]\n",
    "            V_j = V[j * B_c : (j + 1) * B_c]\n",
    "            S_i = scaling * (Q_i @ K_j.T)\n",
    "            m_i = torch.maximum(m_i, S_i.max(dim=-1, keepdim=True).values)\n",
    "            P_i = torch.exp(S_i - m_i)\n",
    "            L_i = torch.exp(last_m_i - m_i) * L_i + P_i.sum(dim=-1, keepdim=True)\n",
    "            O_i = torch.exp(last_m_i - m_i) * O_i + P_i @ V_j\n",
    "            last_m_i = m_i\n",
    "        O_i = (1.0 / L_i) * O_i\n",
    "        L_i = m_i + torch.log(L_i)\n",
    "        O[i * B_r : (i + 1) * B_r] = O_i\n",
    "        L[i * B_r : (i + 1) * B_r] = L_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max absolute difference:\n",
      "O:  tensor(4.1723e-07, device='cuda:0')\n",
      "L:  tensor(9.5367e-07, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "O_torch_loop = torch.zeros(N_out, d)\n",
    "L_torch_loop = torch.zeros(N_out, 1)\n",
    "\n",
    "flash_attention_torch(Q.to(\"cpu\"), K.to(\"cpu\"), V.to(\"cpu\"), O_torch_loop, L_torch_loop, N_inp, N_out, d)\n",
    "\n",
    "check_diff(\n",
    "    O_torch_loop.to(\"cuda\"), \n",
    "    O_expected,\n",
    "    L_torch_loop.to(\"cuda\"),\n",
    "    L_expected\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numba\n",
    "\n",
    "Tiling strategy: each thread computes one value in\n",
    "\n",
    "### All arrays in shared memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.cuda.jit\n",
    "def flash_attention_numba_all_smem(Q, K, V, scaling: numba.float32, L, O, N_out, N_inp):\n",
    "    B_c = 16\n",
    "    B_r = 16\n",
    "    T_c = (N_inp + B_c - 1) // B_c\n",
    "    T_r = (N_out + B_r - 1) // B_r\n",
    "    inp_dtype = K.dtype\n",
    "    tid_x = numba.cuda.threadIdx.x\n",
    "    tid_y = numba.cuda.threadIdx.y\n",
    "\n",
    "    Q_i = numba.cuda.shared.array((B_r, d), inp_dtype)\n",
    "    K_j = numba.cuda.shared.array((B_c, d), inp_dtype)\n",
    "    V_j = numba.cuda.shared.array((B_c, d), inp_dtype)\n",
    "    S = numba.cuda.shared.array((B_r, B_c), inp_dtype)\n",
    "    # These can be in registers but wont fit too large\n",
    "    l_i = numba.cuda.shared.array((B_r,), inp_dtype)\n",
    "    m_i = numba.cuda.shared.array((B_r,), inp_dtype)\n",
    "    O_i = numba.cuda.shared.array((B_r, d), inp_dtype)\n",
    "\n",
    "                 \n",
    "    for i in range(T_r):\n",
    "        for ii in range(tid_y, B_r, numba.cuda.blockDim.y):\n",
    "            for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                Q_i[ii, dd] = Q[ii + i * B_r, dd]\n",
    "                O_i[ii, dd] = 0\n",
    "            l_i[ii] = 0\n",
    "            m_i[ii] = -math.inf\n",
    "        numba.cuda.syncthreads()\n",
    "\n",
    "        for j in range(T_c):\n",
    "            for jj in range(tid_y, B_c, numba.cuda.blockDim.y):\n",
    "                for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                    K_j[jj, dd] = K[jj + j * B_c, dd]\n",
    "                    V_j[jj, dd] = V[jj + j * B_c, dd]\n",
    "\n",
    "            # S[ii][jj] = scaling * (Q_i @ K_j.T)\n",
    "            numba.cuda.syncthreads()\n",
    "            for ii in range(tid_x, B_r, numba.cuda.blockDim.x):\n",
    "                for jj in range(tid_y, B_c, numba.cuda.blockDim.y):\n",
    "                    S_ij = 0\n",
    "                    for dd in range(d):\n",
    "                        S_ij += Q_i[ii, dd] * K_j[jj, dd]\n",
    "                    S_ij = scaling * S_ij\n",
    "                    S[ii][jj] = S_ij\n",
    "\n",
    "            numba.cuda.syncthreads()\n",
    "            for ii in range(tid_y, B_r, numba.cuda.blockDim.y):\n",
    "                m = m_i[ii]\n",
    "                last_m = m\n",
    "                for jj in range(B_c):\n",
    "                    m = max(m, S[ii][jj])\n",
    "                m_i[ii] = m\n",
    "                l = math.exp(last_m - m) * l_i[ii]\n",
    "\n",
    "                for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                    O_i[ii, dd] *= math.exp(last_m - m)\n",
    "                for jj in range(B_c):\n",
    "                    P_ij = math.exp(S[ii][jj] - m)  # Cache...\n",
    "                    l += P_ij\n",
    "                    for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                        O_i[ii, dd] += P_ij * V_j[jj, dd]\n",
    "                l_i[ii] = l\n",
    "                \n",
    "        numba.cuda.syncthreads()\n",
    "        for ii in range(tid_y, B_r, numba.cuda.blockDim.y):  \n",
    "            for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                O[ii + i * B_r, dd] = O_i[ii, dd] / l_i[ii]\n",
    "            L[ii + i * B_r] = m_i[ii] + math.log(l_i[ii])\n",
    "        numba.cuda.syncthreads() \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max absolute difference:\n",
      "O:  tensor(2.9802e-07, device='cuda:0')\n",
      "L:  tensor(4.7684e-07, device='cuda:0')\n",
      "Max absolute difference:\n",
      "O:  tensor(4.4703e-07, device='cuda:0')\n",
      "L:  tensor(1.4305e-06, device='cuda:0')\n",
      "Max absolute difference:\n",
      "O:  tensor(4.7684e-07, device='cuda:0')\n",
      "L:  tensor(4.7684e-07, device='cuda:0')\n",
      "Max absolute difference:\n",
      "O:  tensor(4.4703e-07, device='cuda:0')\n",
      "L:  tensor(1.4305e-06, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "block_dim_x = 32\n",
    "block_dim_y = 16\n",
    "\n",
    "for N_inp, N_out in TEST_DIMS:\n",
    "\n",
    "    Q, K, V, scaling, O_expected, L_expected = get_test_tensors(N_inp, N_out, d)\n",
    "    O_all_smem = torch.zeros(N_out, d, device=\"cuda\").contiguous()\n",
    "    L_all_smem = torch.zeros(N_out, device=\"cuda\")\n",
    "    tpb = (block_dim_x, block_dim_y)\n",
    "    grid = (1,)\n",
    "    flash_attention_numba_all_smem[grid, tpb](Q, K, V, scaling, L_all_smem, O_all_smem,  N_out, N_inp)\n",
    "    torch.cuda.synchronize()\n",
    "    try:\n",
    "        check_diff(\n",
    "            O_all_smem, \n",
    "            O_expected,\n",
    "            L_all_smem,\n",
    "            L_expected,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Failed {N_out=}, {N_inp=}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving `m_i`, `l_i`, `O_i` to local arrays\n",
    "\n",
    "Current shared-memory usage across threads:\n",
    "```\n",
    "Shar = (B_r * d * 4) # Q_i\n",
    "+ (B_c * d * 4) # K_j\n",
    "+ (B_c * d * 4) # V_j\n",
    "+ (B_r * B_c * 4) # S\n",
    "= ~25 KB\n",
    "```\n",
    "\n",
    "Current block-shared accumulators (`m_i`, `l_i`, `O_i`):\n",
    "```\n",
    "Loc = 4 * (B_r + B_r + (B_r * d)) = 8320 B ≈ 8 KB\n",
    "```\n",
    "\n",
    "Total shared usage: **~33 KB** (fine for 1 block/SM).\n",
    "\n",
    "---\n",
    "\n",
    "**Idea:** Move `m_i`, `l_i`, `O_i` to *per-thread* locals to fit in registers.\n",
    "\n",
    "Problem: Full-size per-thread arrays would need\n",
    "\n",
    "```\n",
    "Loc * 32 * 16 ≈ 4 MB > 64 KB register file per SM\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Optimization:** With tiling `tpb = (32, 16)`:\n",
    "\n",
    "- Each thread handles only  \n",
    "  `d // blockDim.x = 4` columns in `x`  \n",
    "  `B_r // blockDim.y = 1` row in `y`\n",
    "- So per-thread locals can be much smaller:\n",
    "\n",
    "```python\n",
    "l_i = numba.cuda.local.array((1,), inp_dtype)   # 4 B\n",
    "m_i = numba.cuda.local.array((1,), inp_dtype)   # 4 B\n",
    "O_i = numba.cuda.local.array((4,), inp_dtype)   # 16 B\n",
    "```\n",
    "-> Per-thread = 24 B, per block = 24 * 32 * 16 = 12 KB < 64 KB -> avoid register pressure and spills.\n",
    "\n",
    "\n",
    "This is how we set up `flash_attention_numba` below and the cuda version in `./flash_attention.cu`\n",
    "\n",
    "In the performance section we run `./flash_attention_spilling_from_registers.cu` that fits full arrays in local variables, to show the performance decrease by slowing the kernel by ~2.5×"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_dim_x = 32\n",
    "block_dim_y = 16\n",
    "B_r = 16\n",
    "B_c = 16\n",
    "d_over_dim_x = d // block_dim_x\n",
    "B_r_over_dim_y = B_r // block_dim_y\n",
    "\n",
    "@numba.cuda.jit\n",
    "def flash_attention_numba(Q, K, V, scaling: numba.float32, L, O, N_out, N_inp):\n",
    "    B_c = 16\n",
    "    B_r = 16\n",
    "    T_c = (N_inp + B_c - 1) // B_c\n",
    "    T_r = (N_out + B_r - 1) // B_r\n",
    "    inp_dtype = K.dtype\n",
    "    tid_x = numba.cuda.threadIdx.x\n",
    "    tid_y = numba.cuda.threadIdx.y\n",
    "    dim_y = numba.cuda.blockDim.y\n",
    "    dim_x = numba.cuda.blockDim.x\n",
    "    \n",
    "\n",
    "    Q_i = numba.cuda.shared.array((B_r, d), inp_dtype)\n",
    "    K_j = numba.cuda.shared.array((B_c, d), inp_dtype)\n",
    "    V_j = numba.cuda.shared.array((B_c, d), inp_dtype)\n",
    "    S = numba.cuda.shared.array((B_r, B_c), inp_dtype)\n",
    "\n",
    "    l_i = numba.cuda.local.array((B_r_over_dim_y,), inp_dtype)\n",
    "    m_i = numba.cuda.local.array((B_r_over_dim_y,), inp_dtype)\n",
    "    O_i = numba.cuda.local.array((B_r_over_dim_y, d_over_dim_x), inp_dtype)\n",
    "\n",
    "                 \n",
    "    for i in range(T_r):\n",
    "        for ii in range(tid_y, B_r, dim_y):\n",
    "            for dd in range(tid_x, d, dim_x):\n",
    "                Q_i[ii, dd] = Q[ii + i * B_r, dd]\n",
    "        numba.cuda.syncthreads()\n",
    "\n",
    "        for ii in range(B_r_over_dim_y):\n",
    "            for dd in range(d_over_dim_x):\n",
    "                O_i[ii, dd] = 0\n",
    "            l_i[ii] = 0\n",
    "            m_i[ii] = -math.inf\n",
    "        numba.cuda.syncthreads()\n",
    "\n",
    "        for j in range(T_c):\n",
    "            for jj in range(tid_y, B_c, dim_y):\n",
    "                for dd in range(tid_x, d, dim_x):\n",
    "                    K_j[jj, dd] = K[jj + j * B_c, dd]\n",
    "                    V_j[jj, dd] = V[jj + j * B_c, dd]\n",
    "\n",
    "            # S[ii][jj] = scaling * (Q_i @ K_j.T)\n",
    "            numba.cuda.syncthreads()\n",
    "            for ii in range(tid_x, B_r, dim_x):\n",
    "                for jj in range(tid_y, B_c, dim_y):\n",
    "                    S_ij = 0\n",
    "                    for dd in range(d):\n",
    "                        S_ij += Q_i[ii, dd] * K_j[jj, dd]\n",
    "                    S_ij = scaling * S_ij\n",
    "                    S[ii][jj] = S_ij\n",
    "\n",
    "            numba.cuda.syncthreads()\n",
    "            for ii in range(B_r_over_dim_y):\n",
    "                m = m_i[ii]\n",
    "                last_m = m\n",
    "                for jj in range(B_c):\n",
    "                    m = max(m, S[ii * dim_y + tid_y][jj])\n",
    "                m_i[ii] = m\n",
    "                l = numba.float32(math.exp(last_m - m)) * l_i[ii]\n",
    "\n",
    "                for dd in range(d_over_dim_x):\n",
    "                    O_i[ii, dd] *= numba.float32(math.exp(last_m - m))\n",
    "                for jj in range(B_c):\n",
    "                    P_ij = numba.float32(math.exp(S[ii * dim_y + tid_y][jj] - m))\n",
    "                    l += P_ij\n",
    "                    for dd in range(d_over_dim_x):\n",
    "                        O_i[ii, dd] += P_ij * V_j[jj, dd * dim_x + tid_x]\n",
    "                l_i[ii] = l\n",
    "                \n",
    "        numba.cuda.syncthreads()\n",
    "        for ii in range(B_r_over_dim_y):  \n",
    "            for dd in range(d_over_dim_x):\n",
    "                O[ii * dim_y + tid_y + i * B_r, dd * dim_x + tid_x] = O_i[ii, dd] / l_i[ii]\n",
    "            L[ii * dim_y + tid_y + i * B_r] = m_i[ii] + numba.float32(math.log(l_i[ii]))\n",
    "        numba.cuda.syncthreads() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max absolute difference:\n",
      "O:  tensor(4.7684e-07, device='cuda:0')\n",
      "L:  tensor(4.7684e-07, device='cuda:0')\n",
      "Max absolute difference:\n",
      "O:  tensor(5.0664e-07, device='cuda:0')\n",
      "L:  tensor(9.5367e-07, device='cuda:0')\n",
      "Max absolute difference:\n",
      "O:  tensor(3.8743e-07, device='cuda:0')\n",
      "L:  tensor(9.5367e-07, device='cuda:0')\n",
      "Max absolute difference:\n",
      "O:  tensor(7.1526e-07, device='cuda:0')\n",
      "L:  tensor(1.4305e-06, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for N_inp, N_out in TEST_DIMS:\n",
    "\n",
    "    Q, K, V, scaling, O_expected, L_expected = get_test_tensors(N_inp, N_out, d)\n",
    "    O_expected = torch.nn.functional.scaled_dot_product_attention(Q, K, V)\n",
    "    S = (Q @ K.T) * scaling  # shape: (N_out, N_inp)\n",
    "    L_expected = torch.logsumexp(S, dim=-1)\n",
    "\n",
    "\n",
    "    O_all_smem = torch.zeros(N_out, d, device=\"cuda\").contiguous()\n",
    "    L_all_smem = torch.zeros(N_out, device=\"cuda\")\n",
    "    tpb = (block_dim_x, block_dim_y)\n",
    "    grid = (1,)\n",
    "    flash_attention_numba[grid, tpb](Q, K, V, scaling, L_all_smem, O_all_smem,  N_out, N_inp)\n",
    "    torch.cuda.synchronize()\n",
    "    try:\n",
    "        check_diff(\n",
    "            O_all_smem, \n",
    "            O_expected,\n",
    "            L_all_smem,\n",
    "            L_expected,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Failed {N_out=}, {N_inp=}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuda\n",
    "\n",
    "### flash_attention_numba in Cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/zeus/.cache/torch_extensions/py310_cu128 as PyTorch extensions root...\n",
      "The input conditions for extension module flash_attentiontest have changed. Bumping to version 1 and re-building as flash_attentiontest_v1...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/zeus/.cache/torch_extensions/py310_cu128/flash_attentiontest/build.ninja...\n",
      "Building extension module flash_attentiontest_v1...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=flash_attentiontest_v1 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1016\\\" -isystem /home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/include -isystem /home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zeus/miniconda3/envs/cloudspace/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++17 -c /home/zeus/.cache/torch_extensions/py310_cu128/flash_attentiontest/main.cpp -o main.o \n",
      "[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=flash_attentiontest_v1 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1016\\\" -isystem /home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/include -isystem /home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zeus/miniconda3/envs/cloudspace/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -O3 -Xptxas -O3 -Xcompiler -O3 -std=c++17 -c /home/zeus/.cache/torch_extensions/py310_cu128/flash_attentiontest/cuda.cu -o cuda.cuda.o \n",
      "[3/3] c++ main.o cuda.cuda.o -shared -L/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o flash_attentiontest_v1.so\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module flash_attentiontest_v1...\n"
     ]
    }
   ],
   "source": [
    "fname = \"flash_attention\"\n",
    "module_cuda = get_loaded_cuda_module(fname, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max absolute difference:\n",
      "O:  tensor(4.7684e-07, device='cuda:0')\n",
      "L:  tensor(4.7684e-07, device='cuda:0')\n",
      "Max absolute difference:\n",
      "O:  tensor(6.1095e-07, device='cuda:0')\n",
      "L:  tensor(9.5367e-07, device='cuda:0')\n",
      "Max absolute difference:\n",
      "O:  tensor(6.2585e-07, device='cuda:0')\n",
      "L:  tensor(9.5367e-07, device='cuda:0')\n",
      "Max absolute difference:\n",
      "O:  tensor(1.0729e-06, device='cuda:0')\n",
      "L:  tensor(9.5367e-07, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for N_inp, N_out in TEST_DIMS:\n",
    "\n",
    "    Q, K, V, scaling, O_expected, L_expected = get_test_tensors(N_inp, N_out, d)\n",
    "\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    O_move_registers, L_move_registers = getattr(module_cuda, fname)(Q, K, V)\n",
    "    try:\n",
    "        check_diff(\n",
    "            O_move_registers, \n",
    "            O_expected,\n",
    "            L_move_registers,\n",
    "            L_expected,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Failed {N_out=}, {N_inp=}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registers spilling version\n",
    "\n",
    "This is the version loading full arrays as local variables in threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m fname_spill_from_registers \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attention_spilling_from_registers\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m module_cuda_spilling_from_registers \u001b[38;5;241m=\u001b[39m \u001b[43mget_loaded_cuda_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname_spill_from_registers\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 22\u001b[0m, in \u001b[0;36mget_loaded_cuda_module\u001b[0;34m(fname, verbose)\u001b[0m\n\u001b[1;32m     20\u001b[0m cuda_src \u001b[38;5;241m=\u001b[39m cuda_src\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour_function_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, fname)\n\u001b[1;32m     21\u001b[0m cpp_src \u001b[38;5;241m=\u001b[39m get_sig(fname, cuda_src)\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_cuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcuda_src\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcpp_src\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/lectures/lecture_012/../utils.py:38\u001b[0m, in \u001b[0;36mload_cuda\u001b[0;34m(cuda_src, cpp_src, funcs, opt, verbose, name)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: name \u001b[38;5;241m=\u001b[39m funcs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     37\u001b[0m flags \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-O3 -Xptxas -O3 -Xcompiler -O3\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m opt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-O0 -Xptxas -O0 -Xcompiler -O0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_inline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcuda_sources\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcuda_src\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcpp_sources\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcpp_src\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuncs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mextra_cuda_cflags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mflags\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1994\u001b[0m, in \u001b[0;36mload_inline\u001b[0;34m(name, cpp_sources, cuda_sources, sycl_sources, functions, extra_cflags, extra_cuda_cflags, extra_sycl_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, with_sycl, is_python_module, with_pytorch_error_handling, keep_intermediates, use_pch)\u001b[0m\n\u001b[1;32m   1990\u001b[0m     _maybe_write(sycl_source_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(sycl_sources))\n\u001b[1;32m   1992\u001b[0m     sources\u001b[38;5;241m.\u001b[39mappend(sycl_source_path)\n\u001b[0;32m-> 1994\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_jit_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1995\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1996\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1997\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_cflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1998\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_cuda_cflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1999\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_sycl_cflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2000\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_ldflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2001\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_include_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2002\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuild_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2003\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2004\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_cuda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2005\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_sycl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2006\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_python_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2007\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_standalone\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2008\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_intermediates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_intermediates\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/cpp_extension.py:2076\u001b[0m, in \u001b[0;36m_jit_compile\u001b[0;34m(name, sources, extra_cflags, extra_cuda_cflags, extra_sycl_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, with_sycl, is_python_module, is_standalone, keep_intermediates)\u001b[0m\n\u001b[1;32m   2072\u001b[0m                 hipified_sources\u001b[38;5;241m.\u001b[39madd(hipify_result[s_abs]\u001b[38;5;241m.\u001b[39mhipified_path \u001b[38;5;28;01mif\u001b[39;00m s_abs \u001b[38;5;129;01min\u001b[39;00m hipify_result \u001b[38;5;28;01melse\u001b[39;00m s_abs)\n\u001b[1;32m   2074\u001b[0m             sources \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(hipified_sources)\n\u001b[0;32m-> 2076\u001b[0m         \u001b[43m_write_ninja_file_and_build_library\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2077\u001b[0m \u001b[43m            \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2078\u001b[0m \u001b[43m            \u001b[49m\u001b[43msources\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2079\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_cflags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_cflags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2080\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_cuda_cflags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_cuda_cflags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2081\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_sycl_cflags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_sycl_cflags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2082\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_ldflags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_ldflags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2083\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_include_paths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_include_paths\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2084\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbuild_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuild_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2085\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2086\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_cuda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_cuda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2087\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_sycl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_sycl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2088\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_standalone\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_standalone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2089\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m verbose:\n\u001b[1;32m   2090\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo modifications detected for re-loaded extension \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2091\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, skipping build step...\u001b[39m\u001b[38;5;124m'\u001b[39m, file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mstderr)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/cpp_extension.py:2222\u001b[0m, in \u001b[0;36m_write_ninja_file_and_build_library\u001b[0;34m(name, sources, extra_cflags, extra_cuda_cflags, extra_sycl_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, with_sycl, is_standalone)\u001b[0m\n\u001b[1;32m   2220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m   2221\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBuilding extension module \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m'\u001b[39m, file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[0;32m-> 2222\u001b[0m \u001b[43m_run_ninja_build\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuild_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mError building extension \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/cpp_extension.py:2506\u001b[0m, in \u001b[0;36m_run_ninja_build\u001b[0;34m(build_directory, verbose, error_prefix)\u001b[0m\n\u001b[1;32m   2493\u001b[0m     \u001b[38;5;66;03m# Warning: don't pass stdout=None to subprocess.run to get output.\u001b[39;00m\n\u001b[1;32m   2494\u001b[0m     \u001b[38;5;66;03m# subprocess.run assumes that sys.__stdout__ has not been modified and\u001b[39;00m\n\u001b[1;32m   2495\u001b[0m     \u001b[38;5;66;03m# attempts to write to it by default.  However, when we call _run_ninja_build\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2503\u001b[0m     \u001b[38;5;66;03m# To work around this, we pass in the fileno directly and hope that\u001b[39;00m\n\u001b[1;32m   2504\u001b[0m     \u001b[38;5;66;03m# it is valid.\u001b[39;00m\n\u001b[1;32m   2505\u001b[0m     stdout_fileno \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 2506\u001b[0m     \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstdout_fileno\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSTDOUT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuild_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2512\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2513\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m subprocess\u001b[38;5;241m.\u001b[39mCalledProcessError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2514\u001b[0m     \u001b[38;5;66;03m# Python 2 and 3 compatible way of getting the error object.\u001b[39;00m\n\u001b[1;32m   2515\u001b[0m     _, error, _ \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/subprocess.py:505\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 505\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    507\u001b[0m         process\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/subprocess.py:1141\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stdin_write(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout:\n\u001b[0;32m-> 1141\u001b[0m     stdout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fname_spill_from_registers = \"flash_attention_spilling_from_registers\"\n",
    "module_cuda_spilling_from_registers = get_loaded_cuda_module(fname_spill_from_registers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (128) must match the size of tensor b (1024) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m O_cuda_spilling, L_cuda_spilling \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module_cuda_spilling_from_registers, fname_spill_from_registers)(Q, K, V)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mcheck_diff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mO_cuda_spilling\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mL_cuda_spilling\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m bas\n",
      "Cell \u001b[0;32mIn[1], line 26\u001b[0m, in \u001b[0;36mcheck_diff\u001b[0;34m(O, O_expected, L, L_expected, atol)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcheck_diff\u001b[39m(O, O_expected, L\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, L_expected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1e-5\u001b[39m):\n\u001b[0;32m---> 26\u001b[0m     O_diff \u001b[38;5;241m=\u001b[39m (\u001b[43mO\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mO_expected\u001b[49m)\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmax()\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax absolute difference:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m atol:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (128) must match the size of tensor b (1024) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "O_cuda_spilling, L_cuda_spilling = getattr(module_cuda_spilling_from_registers, fname_spill_from_registers)(Q, K, V)\n",
    "check_diff(O_cuda_spilling, L_cuda_spilling)\n",
    "bas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuda-Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvrtcResult.NVRTC_ERROR_COMPILATION\n",
      "nvrtcResult.NVRTC_SUCCESS\n",
      "flash_attention.cu(1): error: this declaration has no storage class or type specifier\n",
      "\n",
      "flash_attention.cu(1): error: A namespace scope variable without memory space annotations (__device__/__constant__/__shared__/__managed__) is considered a host variable, and host variables are not allowed in JIT mode. Consider using -default-device flag to process unannotated namespace scope variables as __device__ variables in JIT mode\n",
      "\n",
      "flash_attention.cu(1): error: expected a \";\"\n",
      "\n",
      "flash_attention.cu(2): error: this declaration has no storage class or type specifier\n",
      "\n",
      "flash_attention.cu(2): error: variable \"constexpr\" has already been defined\n",
      "\n",
      "flash_attention.cu(2): error: A namespace scope variable without memory space annotations (__device__/__constant__/__shared__/__managed__) is considered a host variable, and host variables are not allowed in JIT mode. Consider using -default-device flag to process unannotated namespace scope variables as __device__ variables in JIT mode\n",
      "\n",
      "flash_attention.cu(2): error: expected a \";\"\n",
      "\n",
      "flash_attention.cu(3): error: this declaration has no storage class or type specifier\n",
      "\n",
      "flash_attention.cu(3): error: variable \"constexpr\" has already been defined\n",
      "\n",
      "flash_attention.cu(3): error: A namespace scope variable without memory space annotations (__device__/__constant__/__shared__/__managed__) is considered a host variable, and host variables are not allowed in JIT mode. Consider using -default-device flag to process unannotated namespace scope variables as __device__ variables in JIT mode\n",
      "\n",
      "flash_attention.cu(3): error: expected a \";\"\n",
      "\n",
      "flash_attention.cu(4): error: this declaration has no storage class or type specifier\n",
      "\n",
      "flash_attention.cu(4): error: variable \"constexpr\" has already been defined\n",
      "\n",
      "flash_attention.cu(4): error: A namespace scope variable without memory space annotations (__device__/__constant__/__shared__/__managed__) is considered a host variable, and host variables are not allowed in JIT mode. Consider using -default-device flag to process unannotated namespace scope variables as __device__ variables in JIT mode\n",
      "\n",
      "flash_attention.cu(4): error: expected a \";\"\n",
      "\n",
      "flash_attention.cu(5): error: this declaration has no storage class or type specifier\n",
      "\n",
      "flash_attention.cu(5): error: variable \"constexpr\" has already been defined\n",
      "\n",
      "flash_attention.cu(5): error: A namespace scope variable without memory space annotations (__device__/__constant__/__shared__/__managed__) is considered a host variable, and host variables are not allowed in JIT mode. Consider using -default-device flag to process unannotated namespace scope variables as __device__ variables in JIT mode\n",
      "\n",
      "flash_attention.cu(5): error: expected a \";\"\n",
      "\n",
      "flash_attention.cu(6): error: this declaration has no storage class or type specifier\n",
      "\n",
      "flash_attention.cu(6): error: variable \"constexpr\" has already been defined\n",
      "\n",
      "flash_attention.cu(6): error: A namespace scope variable without memory space annotations (__device__/__constant__/__shared__/__managed__) is considered a host variable, and host variables are not allowed in JIT mode. Consider using -default-device flag to process unannotated namespace scope variables as __device__ variables in JIT mode\n",
      "\n",
      "flash_attention.cu(6): error: expected a \";\"\n",
      "\n",
      "flash_attention.cu(7): error: this declaration has no storage class or type specifier\n",
      "\n",
      "flash_attention.cu(7): error: variable \"constexpr\" has already been defined\n",
      "\n",
      "flash_attention.cu(7): error: A namespace scope variable without memory space annotations (__device__/__constant__/__shared__/__managed__) is considered a host variable, and host variables are not allowed in JIT mode. Consider using -default-device flag to process unannotated namespace scope variables as __device__ variables in JIT mode\n",
      "\n",
      "flash_attention.cu(7): error: expected a \";\"\n",
      "\n",
      "flash_attention.cu(30): error: identifier \"B_r\" is undefined\n",
      "\n",
      "flash_attention.cu(30): error: identifier \"d\" is undefined\n",
      "\n",
      "flash_attention.cu(31): error: identifier \"B_c\" is undefined\n",
      "\n",
      "flash_attention.cu(36): error: identifier \"B_r_over_dim_y\" is undefined\n",
      "\n",
      "flash_attention.cu(38): error: identifier \"d_over_dim_x\" is undefined\n",
      "\n",
      "32 errors detected in the compilation of \"flash_attention.cu\".\n",
      "\u0000\n",
      "CUresult.CUDA_ERROR_INVALID_IMAGE\n",
      "CUresult.CUDA_ERROR_INVALID_HANDLE <CUfunction 0x0>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1269866112, 1269866120, 1269866128, 1269866136, 1269866144, 1270417472,\n",
       "        1269801920, 1269801924, 1269801928], dtype=torch.uint64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create program\n",
    "# conda install conda-forge::cuda-python\n",
    "from cuda import cuda, nvrtc\n",
    "\n",
    "cuda_src_path = f\"./{fname}.cu\"\n",
    "cuda_src = Path(cuda_src_path).read_text()\n",
    "\n",
    "N_inp = 32\n",
    "N_out = 32\n",
    "d = 128\n",
    "B_r, B_c = 16, 16\n",
    "T_r = (N_out + B_r -1) // B_r\n",
    "T_c = (N_inp + B_r -1) // B_c\n",
    "Q, K, V, scale_factor, O_expected, L_expected = get_test_tensors(N_inp, N_out, d)\n",
    "\n",
    "err, prog = nvrtc.nvrtcCreateProgram(str.encode(cuda_src), b\"flash_attention.cu\", 0, [], [])\n",
    "\n",
    "# Compile program\n",
    "min, maj = torch.cuda.get_device_capability()\n",
    "opts = [f\"--gpu-architecture=compute_{min}{maj}\".encode()]\n",
    "err, = nvrtc.nvrtcCompileProgram(prog, len(opts), opts)\n",
    "\n",
    "print(err)\n",
    "\n",
    "# Get PTX from compilation\n",
    "err, ptxSize = nvrtc.nvrtcGetPTXSize(prog)\n",
    "ptx = b\" \" * ptxSize\n",
    "err, = nvrtc.nvrtcGetPTX(prog, ptx)\n",
    "print(err)\n",
    "\n",
    "err, logSize = nvrtc.nvrtcGetProgramLogSize(prog)\n",
    "log = b\" \" * logSize\n",
    "err, = nvrtc.nvrtcGetProgramLog(prog, log)\n",
    "print(log.decode())\n",
    "# print(ptx.decode())\n",
    "\n",
    "# Load PTX as module data and retrieve function\n",
    "err, module = cuda.cuModuleLoadData(ptx)\n",
    "print(err)\n",
    "err, kernel = cuda.cuModuleGetFunction(module, b\"flash_attention_k\")\n",
    "print(err, kernel)\n",
    "\n",
    "# Allocate tensors\n",
    "# S3 = torch.zeros(N_out, N_out, device=\"cuda\")\n",
    "O_cuda_py = torch.zeros(N_out, d, device=\"cuda\")\n",
    "L_cuda_py = torch.zeros(N_out, device=\"cuda\")\n",
    "\n",
    "# To quote the official tutorial: (https://nvidia.github.io/cuda-python/overview.html)\n",
    "# The following code example is not intuitive\n",
    "# Subject to change in a future release\n",
    "\n",
    "int_args = torch.tensor([0, T_r, T_c], dtype=torch.int32)\n",
    "float_args = torch.tensor([scale_factor], dtype=torch.float32)\n",
    "ptr_args = torch.tensor([i.data_ptr() for i in (O_cuda_py, L_cuda_py, K, Q, V)], dtype=torch.uint64)\n",
    "\n",
    "args = torch.tensor([\n",
    "    *(i.data_ptr() for i in ptr_args),\n",
    "    *(i.data_ptr() for i in float_args),\n",
    "    *(i.data_ptr() for i in int_args)], dtype=torch.uint64)\n",
    "\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1937, device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O_expected = torch.nn.functional.scaled_dot_product_attention(Q, K, V)\n",
    "def fn():\n",
    "    err = cuda.cuLaunchKernel(\n",
    "        kernel,\n",
    "        1,  # grid x dim\n",
    "        1,  # grid y dim\n",
    "        1,  # grid z dim\n",
    "        32,  # block x dim\n",
    "        16,  # block y dim\n",
    "        1,  # block z dim\n",
    "        0,  # dynamic shared memory\n",
    "        torch.cuda.current_stream().stream_id,  # stream\n",
    "        args.data_ptr(),  # kernel arguments\n",
    "        0,  # extra (ignore)\n",
    "    )\n",
    "\n",
    "fn()\n",
    "\n",
    "(O_cuda_py - O_expected).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "**********\n",
      "Dimensions: N_out=32, N_inp=32, d=128\n",
      "\n",
      "- Custom Flash Attention: Cuda-python\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.4 µs ± 232 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n\\n**********\\nDimensions: {N_out=}, {N_inp=}, {d=}\")\n",
    "torch.cuda.synchronize()\n",
    "print(\"\\n- Custom Flash Attention: Cuda-python\")\n",
    "%timeit fn(); torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance \n",
    "\n",
    "Run timeit on different dimensions. \n",
    "\n",
    "- Recall that we build the kernel for `d=128` and design it so that it computes the full attention in a single block.\n",
    "\n",
    "- For matrices with small `N_out` this implementation is significantly faster than `scaled_dot_product_attention`.\n",
    "\n",
    "- But by making N larger this implementation slows down dramatically it only uses a single block.\n",
    "\n",
    "- Note that the register splilling version is much slower even compared to `scaled_dot_product_attention`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "**********\n",
      "Dimensions: N_out=32, N_inp=32, d=128\n",
      "\n",
      "- Torch scaled_dot_product_attention\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169 µs ± 1.74 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "\n",
      "- Custom Flash Attention\n",
      "97.8 µs ± 333 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "\n",
      "- Custom Flash Attention: spill from registers\n",
      "351 µs ± 1.11 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "\n",
      "\n",
      "**********\n",
      "Dimensions: N_out=32, N_inp=64, d=128\n",
      "\n",
      "- Torch scaled_dot_product_attention\n",
      "170 µs ± 5 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "\n",
      "- Custom Flash Attention\n",
      "144 µs ± 2.27 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "\n",
      "- Custom Flash Attention: spill from registers\n",
      "644 µs ± 939 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "\n",
      "\n",
      "**********\n",
      "Dimensions: N_out=128, N_inp=128, d=128\n",
      "\n",
      "- Torch scaled_dot_product_attention\n",
      "170 µs ± 1.92 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "\n",
      "- Custom Flash Attention\n",
      "796 µs ± 939 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "\n",
      "- Custom Flash Attention: spill from registers\n",
      "4.77 ms ± 12.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "\n",
      "**********\n",
      "Dimensions: N_out=512, N_inp=512, d=128\n",
      "\n",
      "- Torch scaled_dot_product_attention\n",
      "205 µs ± 5.35 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "\n",
      "- Custom Flash Attention\n",
      "11.7 ms ± 5.35 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "- Custom Flash Attention: spill from registers\n",
      "74.8 ms ± 24.3 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "TEST_DIMS = [\n",
    "    (32, 32),\n",
    "    (64, 32),\n",
    "    (128, 128),\n",
    "    (512, 512),\n",
    "]\n",
    "for N_inp, N_out in TEST_DIMS:\n",
    "    Q, K, V, _ = get_test_tensors(N_inp, N_out, d)\n",
    "    print(f\"\\n\\n**********\\nDimensions: {N_out=}, {N_inp=}, {d=}\")\n",
    "    torch.cuda.synchronize()\n",
    "    print(\"\\n- Torch scaled_dot_product_attention\")\n",
    "    %timeit torch.nn.functional.scaled_dot_product_attention(Q, K, V); torch.cuda.synchronize()\n",
    "    print(\"\\n- Custom Flash Attention\")\n",
    "    %timeit getattr(module_cuda, fname)(Q, K, V); torch.cuda.synchronize()\n",
    "    print(\"\\n- Custom Flash Attention: spill from registers\")\n",
    "    %timeit getattr(module_cuda_spilling_from_registers, fname_spill_from_registers)(Q, K, V); torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profile\n",
    "\n",
    "We can compare the performance of the kernel with and without register spilling as follows:\n",
    "```\n",
    "# Get ptx files\n",
    "nvcc -arch=sm_80 -ptx flash_attention.cu -o flash_attention.ptx\n",
    "nvcc -arch=sm_80 -ptx flash_attention_spilling_from_registers.cu -o flash_attention_spilling_from_registers.ptx\n",
    "\n",
    "# Get ncu metrics\n",
    "nvcc -O3 -o test_attention main.cu flash_attention.cu flash_attention_spilling_from_registers.cu\n",
    "ncu ./test_attention\n",
    "```\n",
    "\n",
    "### PTX comparison\n",
    "\n",
    "TODO\n",
    "\n",
    "\n",
    "### Nsight Compute Comparison\n",
    "\n",
    "| **Metric** | **Spilling Kernel (`flash_attention_spilling_from_registers_k`)** | **Non-Spilling Kernel (`flash_attention_k`)** | **Difference / Interpretation** |\n",
    "|------------|------------------------------------------------------------------|-----------------------------------------------|---------------------------------|\n",
    "| **Duration** | **13.02 ms** | **2.10 ms** | Spilling kernel is ~6× slower. |\n",
    "| **Elapsed Cycles** | 7.6 M | 1.2 M | Extra cycles lost to spills. |\n",
    "| **Compute (SM) Throughput** | **0.26%** | **1.20%** | 5× higher compute utilization in non-spilling kernel. |\n",
    "| **L2 Cache Throughput** | **1.71%** | **0.56%** | Spilling kernel hits L2 more — spilled registers go to local memory via L2. |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "flash_attention_spilling_from_registers_k (1, 1, 1)x(32, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
    "    Section: GPU Speed Of Light Throughput\n",
    "    ----------------------- ------------- ------------\n",
    "    Metric Name               Metric Unit Metric Value\n",
    "    ----------------------- ------------- ------------\n",
    "    DRAM Frequency          cycle/nsecond         5.00\n",
    "    SM Frequency            cycle/usecond       585.15\n",
    "    Elapsed Cycles                  cycle      7619363\n",
    "    Memory Throughput                   %         1.86\n",
    "    DRAM Throughput                     %         0.01\n",
    "    Duration                      msecond        13.02\n",
    "    L1/TEX Cache Throughput             %        74.55\n",
    "    L2 Cache Throughput                 %         1.71\n",
    "    SM Active Cycles                cycle    190430.67\n",
    "    Compute (SM) Throughput             %         0.26\n",
    "    ----------------------- ------------- ------------\n",
    "\n",
    "    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
    "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
    "\n",
    "    Section: Launch Statistics\n",
    "    -------------------------------- --------------- ---------------\n",
    "    Metric Name                          Metric Unit    Metric Value\n",
    "    -------------------------------- --------------- ---------------\n",
    "    Block Size                                                   512\n",
    "    Function Cache Configuration                     CachePreferNone\n",
    "    Grid Size                                                      1\n",
    "    Registers Per Thread             register/thread              60\n",
    "    Shared Memory Configuration Size           Kbyte           65.54\n",
    "    Driver Shared Memory Per Block        byte/block               0\n",
    "    Dynamic Shared Memory Per Block       byte/block               0\n",
    "    Static Shared Memory Per Block       Kbyte/block           25.60\n",
    "    Threads                                   thread             512\n",
    "    Waves Per SM                                                0.01\n",
    "    -------------------------------- --------------- ---------------\n",
    "\n",
    "    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
    "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
    "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
    "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
    "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
    "          description for more details on launch configurations.                                                        \n",
    "\n",
    "    Section: Occupancy\n",
    "    ------------------------------- ----------- ------------\n",
    "    Metric Name                     Metric Unit Metric Value\n",
    "    ------------------------------- ----------- ------------\n",
    "    Block Limit SM                        block           16\n",
    "    Block Limit Registers                 block            2\n",
    "    Block Limit Shared Mem                block            2\n",
    "    Block Limit Warps                     block            2\n",
    "    Theoretical Active Warps per SM        warp           32\n",
    "    Theoretical Occupancy                     %          100\n",
    "    Achieved Occupancy                        %        50.00\n",
    "    Achieved Active Warps Per SM           warp        16.00\n",
    "    ------------------------------- ----------- ------------\n",
    "\n",
    "    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     \n",
    "          theoretical (100.0%) and measured achieved occupancy (50.0%) can be the result of warp scheduling overheads   \n",
    "          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    \n",
    "          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                \n",
    "          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n",
    "          optimizing occupancy.                                                                                         \n",
    "\n",
    "  flash_attention_k (1, 1, 1)x(32, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
    "    Section: GPU Speed Of Light Throughput\n",
    "    ----------------------- ------------- ------------\n",
    "    Metric Name               Metric Unit Metric Value\n",
    "    ----------------------- ------------- ------------\n",
    "    DRAM Frequency          cycle/nsecond         4.99\n",
    "    SM Frequency            cycle/usecond       584.08\n",
    "    Elapsed Cycles                  cycle      1225715\n",
    "    Memory Throughput                   %         1.72\n",
    "    DRAM Throughput                     %         0.04\n",
    "    Duration                      msecond         2.10\n",
    "    L1/TEX Cache Throughput             %        68.85\n",
    "    L2 Cache Throughput                 %         0.56\n",
    "    SM Active Cycles                cycle     30658.20\n",
    "    Compute (SM) Throughput             %         1.20\n",
    "    ----------------------- ------------- ------------\n",
    "\n",
    "    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
    "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
    "\n",
    "    Section: Launch Statistics\n",
    "    -------------------------------- --------------- ---------------\n",
    "    Metric Name                          Metric Unit    Metric Value\n",
    "    -------------------------------- --------------- ---------------\n",
    "    Block Size                                                   512\n",
    "    Function Cache Configuration                     CachePreferNone\n",
    "    Grid Size                                                      1\n",
    "    Registers Per Thread             register/thread              62\n",
    "    Shared Memory Configuration Size           Kbyte           65.54\n",
    "    Driver Shared Memory Per Block        byte/block               0\n",
    "    Dynamic Shared Memory Per Block       byte/block               0\n",
    "    Static Shared Memory Per Block       Kbyte/block           25.60\n",
    "    Threads                                   thread             512\n",
    "    Waves Per SM                                                0.01\n",
    "    -------------------------------- --------------- ---------------\n",
    "\n",
    "    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
    "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
    "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
    "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
    "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
    "          description for more details on launch configurations.                                                        \n",
    "\n",
    "    Section: Occupancy\n",
    "    ------------------------------- ----------- ------------\n",
    "    Metric Name                     Metric Unit Metric Value\n",
    "    ------------------------------- ----------- ------------\n",
    "    Block Limit SM                        block           16\n",
    "    Block Limit Registers                 block            2\n",
    "    Block Limit Shared Mem                block            2\n",
    "    Block Limit Warps                     block            2\n",
    "    Theoretical Active Warps per SM        warp           32\n",
    "    Theoretical Occupancy                     %          100\n",
    "    Achieved Occupancy                        %        50.00\n",
    "    Achieved Active Warps Per SM           warp        16.00\n",
    "    ------------------------------- ----------- ------------\n",
    "\n",
    "    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     \n",
    "          theoretical (100.0%) and measured achieved occupancy (50.0%) can be the result of warp scheduling overheads   \n",
    "          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    \n",
    "          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                \n",
    "          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n",
    "          optimizing occupancy.                                                                                         \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thunder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'thunder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mthunder\u001b[39;00m\n\u001b[1;32m      3\u001b[0m attn_ex \u001b[38;5;241m=\u001b[39m thunder\u001b[38;5;241m.\u001b[39mextend\u001b[38;5;241m.\u001b[39mOperatorExecutor(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattn_ex\u001b[39m\u001b[38;5;124m'\u001b[39m, version\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m      4\u001b[0m thunder\u001b[38;5;241m.\u001b[39madd_default_executor(attn_ex)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'thunder'"
     ]
    }
   ],
   "source": [
    "import thunder\n",
    "\n",
    "attn_ex = thunder.extend.OperatorExecutor('attn_ex', version=0.01)\n",
    "thunder.add_default_executor(attn_ex)\n",
    "\n",
    "[attn_ex, attn_ex, sdpa, nvfuser]\n",
    "\n",
    "def my_attn_impl(query, key, value, scale):\n",
    "    n_out, d = query.shape\n",
    "\n",
    "    # S3 = torch.zeros(N_out, N_out, device=\"cuda\")\n",
    "    O3 = torch.zeros(N_out, d, device=\"cuda\")\n",
    "    L3 = torch.zeros(N_out, device=\"cuda\")\n",
    "\n",
    "    B_c = 16\n",
    "    B_r = 16\n",
    "    T_c = (N_inp + B_c - 1) // B_c\n",
    "    T_r = (N_out + B_r - 1) // B_r\n",
    "\n",
    "    int_args = torch.tensor([N_out, T_r, T_c], dtype=torch.int32)\n",
    "    float_args = torch.tensor([scale_factor], dtype=torch.float32)\n",
    "    ptr_args = torch.tensor([i.data_ptr() for i in (O3, L3, key, query, value)], dtype=torch.uint64)\n",
    "\n",
    "    args = torch.tensor([\n",
    "        *(i.data_ptr() for i in ptr_args),\n",
    "        *(i.data_ptr() for i in float_args),\n",
    "        *(i.data_ptr() for i in int_args)], dtype=torch.uint64\n",
    "    )\n",
    "\n",
    "    err, _ = cuda.cuLaunchKernel(\n",
    "        kernel,\n",
    "        1,  # grid x dim\n",
    "        1,  # grid y dim\n",
    "        1,  # grid z dim\n",
    "        32, # block x dim\n",
    "        32, # block y dim\n",
    "        1,  # block z dim\n",
    "        0,  # dynamic shared memory\n",
    "        torch.cuda.current_stream().stream_id,  # stream\n",
    "        args.data_ptr(),  # kernel arguments\n",
    "        0,  # extra (ignore)\n",
    "    )\n",
    "    assert err == cuda.CUresult.CUDA_SUCCESS, err\n",
    "    return O3, L3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'attn_ex' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmy_attn_meta\u001b[39m(query, key, value, scale):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m thunder\u001b[38;5;241m.\u001b[39mTensorProxy(like\u001b[38;5;241m=\u001b[39mquery), thunder\u001b[38;5;241m.\u001b[39mTensorProxy(like\u001b[38;5;241m=\u001b[39mquery, shape\u001b[38;5;241m=\u001b[39m(query\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],))\n\u001b[0;32m----> 6\u001b[0m my_attn \u001b[38;5;241m=\u001b[39m \u001b[43mattn_ex\u001b[49m\u001b[38;5;241m.\u001b[39mregister_operator(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmy_attn\u001b[39m\u001b[38;5;124m'\u001b[39m, meta\u001b[38;5;241m=\u001b[39mmy_attn_meta, fn\u001b[38;5;241m=\u001b[39mmy_attn_impl)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'attn_ex' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "## Register our implementation as an operator\n",
    "\n",
    "def my_attn_meta(query, key, value, scale):\n",
    "    return thunder.TensorProxy(like=query), thunder.TensorProxy(like=query, shape=(query.shape[:-1],))\n",
    "\n",
    "my_attn = attn_ex.register_operator('my_attn', meta=my_attn_meta, fn=my_attn_impl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_attn_checker(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False, scale=None):\n",
    "    if attn_mask is not None or dropout_p == 0.0 or is_causal:\n",
    "        return False\n",
    "    if len(query.shape) > 2:\n",
    "            return (query.device.device_type == thunder.devices.DeviceType.CUDA and\n",
    "                key.device == query.device and\n",
    "                value.device == query.device)\n",
    "    return False\n",
    "\n",
    "def my_attn_transform(query, key, value, attn_masks=None, dropout_p=0.0, is_causal=False, scale=None):\n",
    "    if scale is None:\n",
    "        scale = query.size(-1) ** -0.5\n",
    "    out = my_attn(query, key, value, scale)\n",
    "    return out[0]\n",
    "\n",
    "attn_ex.register_implementation(thunder.torch.scaled_dot_product_attention, checker=my_attn_checker,\n",
    "                                  execution_transform=my_attn_transform)\n",
    "\n",
    "def test_fn(query, key, value):\n",
    "        return torch.nn.functional.scaled_dot_product_attention(query, key, value, is_causal=False)\n",
    "\n",
    "jfn = thunder.jit(test_fn)\n",
    "\n",
    "print((jfn(qc, kc, vc) - test_fn(qc, kc, vc)).abs().max())\n",
    "print(thunder.last_traces(jfn)[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuda info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PyTorch CUDA Info ===\n",
      "PyTorch version: 2.7.1+cu128\n",
      "CUDA available: True\n",
      "CUDA version: 12.8\n",
      "cuDNN version: 90701\n",
      "Number of GPUs: 1\n",
      "  GPU 0: Tesla T4\n",
      "    Current device: 0\n",
      "    Memory allocated: 8.92 MB\n",
      "    Memory cached   : 27.26 MB\n",
      "\n",
      "=== nvidia-smi Info (if available) ===\n",
      "Sat Aug 16 16:59:12 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.261.03             Driver Version: 535.261.03   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       Off | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   42C    P0              34W /  70W |    439MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "print_cuda_info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
